2025-03-30 19:25:18,207 - utils - INFO - Using device: cuda
2025-03-30 19:25:18,210 - pretrained_eval - INFO - Initial pretrained_eval device setting: cuda
2025-03-30 19:25:18,708 - speaker_verification.pretrained_eval - INFO - Initial pretrained_eval device setting: cuda
2025-03-30 19:25:18,713 - finetune - INFO - Using device: cuda
2025-03-30 19:25:18,713 - __main__ - INFO - Using device: cuda
2025-03-30 19:25:18,714 - __main__ - INFO - Examining saved model: models/speaker_verification/wavlm_ft/best_model.pt
2025-03-30 19:25:19,061 - __main__ - INFO - State dict contains 345 keys
2025-03-30 19:25:19,061 - __main__ - INFO - Found 96 keys containing 'lora'
2025-03-30 19:25:19,107 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight: sum=614.963989, norm=6.937425
2025-03-30 19:25:19,107 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight: sum=97.331726, norm=1.102997
2025-03-30 19:25:19,108 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_A.default.weight: sum=623.872070, norm=7.046288
2025-03-30 19:25:19,108 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight: sum=98.443237, norm=1.112324
2025-03-30 19:25:19,108 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_A.default.weight: sum=603.753601, norm=6.847447
2025-03-30 19:25:19,108 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight: sum=97.832474, norm=1.103037
2025-03-30 19:25:19,109 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_A.default.weight: sum=614.221863, norm=6.945014
2025-03-30 19:25:19,109 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight: sum=99.093399, norm=1.119472
2025-03-30 19:25:19,109 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_A.default.weight: sum=605.234131, norm=6.856238
2025-03-30 19:25:19,109 - __main__ - INFO - LoRA param base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight: sum=97.999046, norm=1.104239
2025-03-30 19:25:19,110 - __main__ - INFO - Comparing embeddings using wavlm_base_plus on data/vox1/wav\id10270/x6uYqmx31kE/00001.wav
2025-03-30 19:25:19,110 - pretrained_eval - INFO - Using device for model loading: cuda
2025-03-30 19:25:19,110 - pretrained_eval - INFO - CUDA is available. Device count: 1
2025-03-30 19:25:19,110 - pretrained_eval - INFO - Loading pretrained model: microsoft/wavlm-base-plus
2025-03-30 19:25:21,600 - pretrained_eval - INFO - Model moved to cuda
2025-03-30 19:25:22,163 - __main__ - INFO - Pretrained Embedding Norm: 1.000000
2025-03-30 19:25:22,163 - pretrained_eval - INFO - Using device for model loading: cuda
2025-03-30 19:25:22,163 - pretrained_eval - INFO - CUDA is available. Device count: 1
2025-03-30 19:25:22,163 - pretrained_eval - INFO - Loading pretrained model: microsoft/wavlm-base-plus
2025-03-30 19:25:29,909 - pretrained_eval - INFO - Model moved to cuda
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,139 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,140 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,141 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,142 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,143 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:30,146 - finetune - INFO - Trainable params: 1179648 || All params: 95638384 || Trainable%: 1.2334%
2025-03-30 19:25:30,146 - __main__ - INFO - Loading weights from models/speaker_verification/wavlm_ft/best_model.pt into dynamic LoRA model
2025-03-30 19:25:30,643 - __main__ - INFO - Weights loaded successfully into dynamic LoRA model.
2025-03-30 19:25:30,644 - finetune - INFO - Verifying LoRA adapter activation...
2025-03-30 19:25:30,644 - finetune - INFO - LoRA verification complete
2025-03-30 19:25:30,842 - __main__ - INFO - Finetuned (Dynamic) Embedding Norm: 1.000000
2025-03-30 19:25:30,843 - pretrained_eval - INFO - Using device for model loading: cuda
2025-03-30 19:25:30,843 - pretrained_eval - INFO - CUDA is available. Device count: 1
2025-03-30 19:25:30,843 - pretrained_eval - INFO - Loading pretrained model: microsoft/wavlm-base-plus
2025-03-30 19:25:32,039 - pretrained_eval - INFO - Model moved to cuda
2025-03-30 19:25:32,092 - __main__ - INFO - Loading weights from models/speaker_verification/wavlm_ft/best_model.pt into model before merging
2025-03-30 19:25:32,380 - __main__ - INFO - LoRA weights loaded for merging. Missing: 344, Unexpected: 96
2025-03-30 19:25:32,380 - __main__ - WARNING - Some LoRA weights seem to be missing during merge load: ['base_model.model.masked_spec_embed', 'base_model.model.feature_extractor.conv_layers.0.conv.weight', 'base_model.model.feature_extractor.conv_layers.0.layer_norm.weight', 'base_model.model.feature_extractor.conv_layers.0.layer_norm.bias', 'base_model.model.feature_extractor.conv_layers.1.conv.weight', 'base_model.model.feature_extractor.conv_layers.2.conv.weight', 'base_model.model.feature_extractor.conv_layers.3.conv.weight', 'base_model.model.feature_extractor.conv_layers.4.conv.weight', 'base_model.model.feature_extractor.conv_layers.5.conv.weight', 'base_model.model.feature_extractor.conv_layers.6.conv.weight', 'base_model.model.feature_projection.layer_norm.weight', 'base_model.model.feature_projection.layer_norm.bias', 'base_model.model.feature_projection.projection.weight', 'base_model.model.feature_projection.projection.bias', 'base_model.model.encoder.pos_conv_embed.conv.bias', 'base_model.model.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'base_model.model.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'base_model.model.encoder.layer_norm.weight', 'base_model.model.encoder.layer_norm.bias', 'base_model.model.encoder.layers.0.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.0.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.0.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.0.attention.rel_attn_embed.weight', 'base_model.model.encoder.layers.0.layer_norm.weight', 'base_model.model.encoder.layers.0.layer_norm.bias', 'base_model.model.encoder.layers.0.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.0.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.0.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.0.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.0.final_layer_norm.weight', 'base_model.model.encoder.layers.0.final_layer_norm.bias', 'base_model.model.encoder.layers.1.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.1.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.1.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.1.layer_norm.weight', 'base_model.model.encoder.layers.1.layer_norm.bias', 'base_model.model.encoder.layers.1.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.1.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.1.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.1.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.1.final_layer_norm.weight', 'base_model.model.encoder.layers.1.final_layer_norm.bias', 'base_model.model.encoder.layers.2.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.2.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.2.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.2.layer_norm.weight', 'base_model.model.encoder.layers.2.layer_norm.bias', 'base_model.model.encoder.layers.2.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.2.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.2.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.2.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.2.final_layer_norm.weight', 'base_model.model.encoder.layers.2.final_layer_norm.bias', 'base_model.model.encoder.layers.3.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.3.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.3.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.3.layer_norm.weight', 'base_model.model.encoder.layers.3.layer_norm.bias', 'base_model.model.encoder.layers.3.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.3.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.3.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.3.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.3.final_layer_norm.weight', 'base_model.model.encoder.layers.3.final_layer_norm.bias', 'base_model.model.encoder.layers.4.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.4.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.4.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.4.layer_norm.weight', 'base_model.model.encoder.layers.4.layer_norm.bias', 'base_model.model.encoder.layers.4.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.4.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.4.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.4.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.4.final_layer_norm.weight', 'base_model.model.encoder.layers.4.final_layer_norm.bias', 'base_model.model.encoder.layers.5.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.5.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.5.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.5.layer_norm.weight', 'base_model.model.encoder.layers.5.layer_norm.bias', 'base_model.model.encoder.layers.5.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.5.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.5.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.5.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.5.final_layer_norm.weight', 'base_model.model.encoder.layers.5.final_layer_norm.bias', 'base_model.model.encoder.layers.6.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.6.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.6.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.6.layer_norm.weight', 'base_model.model.encoder.layers.6.layer_norm.bias', 'base_model.model.encoder.layers.6.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.6.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.6.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.6.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.6.final_layer_norm.weight', 'base_model.model.encoder.layers.6.final_layer_norm.bias', 'base_model.model.encoder.layers.7.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.7.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.7.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.7.layer_norm.weight', 'base_model.model.encoder.layers.7.layer_norm.bias', 'base_model.model.encoder.layers.7.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.7.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.7.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.7.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.7.final_layer_norm.weight', 'base_model.model.encoder.layers.7.final_layer_norm.bias', 'base_model.model.encoder.layers.8.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.8.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.8.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.8.layer_norm.weight', 'base_model.model.encoder.layers.8.layer_norm.bias', 'base_model.model.encoder.layers.8.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.8.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.8.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.8.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.8.final_layer_norm.weight', 'base_model.model.encoder.layers.8.final_layer_norm.bias', 'base_model.model.encoder.layers.9.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.9.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.9.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.9.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.9.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.9.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.9.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.9.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.9.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.9.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.9.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.9.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.9.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.9.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.9.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.9.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.9.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.9.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.9.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.9.layer_norm.weight', 'base_model.model.encoder.layers.9.layer_norm.bias', 'base_model.model.encoder.layers.9.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.9.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.9.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.9.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.9.final_layer_norm.weight', 'base_model.model.encoder.layers.9.final_layer_norm.bias', 'base_model.model.encoder.layers.10.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.10.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.10.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.10.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.10.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.10.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.10.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.10.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.10.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.10.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.10.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.10.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.10.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.10.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.10.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.10.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.10.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.10.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.10.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.10.layer_norm.weight', 'base_model.model.encoder.layers.10.layer_norm.bias', 'base_model.model.encoder.layers.10.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.10.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.10.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.10.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.10.final_layer_norm.weight', 'base_model.model.encoder.layers.10.final_layer_norm.bias', 'base_model.model.encoder.layers.11.attention.gru_rel_pos_const', 'base_model.model.encoder.layers.11.attention.k_proj.base_layer.weight', 'base_model.model.encoder.layers.11.attention.k_proj.base_layer.bias', 'base_model.model.encoder.layers.11.attention.k_proj.lora_A.default.weight', 'base_model.model.encoder.layers.11.attention.k_proj.lora_B.default.weight', 'base_model.model.encoder.layers.11.attention.v_proj.base_layer.weight', 'base_model.model.encoder.layers.11.attention.v_proj.base_layer.bias', 'base_model.model.encoder.layers.11.attention.v_proj.lora_A.default.weight', 'base_model.model.encoder.layers.11.attention.v_proj.lora_B.default.weight', 'base_model.model.encoder.layers.11.attention.q_proj.base_layer.weight', 'base_model.model.encoder.layers.11.attention.q_proj.base_layer.bias', 'base_model.model.encoder.layers.11.attention.q_proj.lora_A.default.weight', 'base_model.model.encoder.layers.11.attention.q_proj.lora_B.default.weight', 'base_model.model.encoder.layers.11.attention.out_proj.base_layer.weight', 'base_model.model.encoder.layers.11.attention.out_proj.base_layer.bias', 'base_model.model.encoder.layers.11.attention.out_proj.lora_A.default.weight', 'base_model.model.encoder.layers.11.attention.out_proj.lora_B.default.weight', 'base_model.model.encoder.layers.11.attention.gru_rel_pos_linear.weight', 'base_model.model.encoder.layers.11.attention.gru_rel_pos_linear.bias', 'base_model.model.encoder.layers.11.layer_norm.weight', 'base_model.model.encoder.layers.11.layer_norm.bias', 'base_model.model.encoder.layers.11.feed_forward.intermediate_dense.weight', 'base_model.model.encoder.layers.11.feed_forward.intermediate_dense.bias', 'base_model.model.encoder.layers.11.feed_forward.output_dense.weight', 'base_model.model.encoder.layers.11.feed_forward.output_dense.bias', 'base_model.model.encoder.layers.11.final_layer_norm.weight', 'base_model.model.encoder.layers.11.final_layer_norm.bias']
2025-03-30 19:25:32,380 - __main__ - INFO - Merging LoRA weights...
2025-03-30 19:25:32,413 - __main__ - INFO - LoRA weights merged successfully.
2025-03-30 19:25:32,453 - __main__ - INFO - Finetuned (Merged) Embedding Norm: 1.000000
2025-03-30 19:25:32,454 - __main__ - INFO - --- Dynamic LoRA Comparison ---
2025-03-30 19:25:32,454 - __main__ - INFO - Embedding difference (L2 norm): 0.000000
2025-03-30 19:25:32,454 - __main__ - INFO - Embedding similarity (cosine): 1.000000
2025-03-30 19:25:32,454 - __main__ - INFO - --- Merged LoRA Comparison ---
2025-03-30 19:25:32,454 - __main__ - INFO - Embedding difference (L2 norm): 0.000000
2025-03-30 19:25:32,454 - __main__ - INFO - Embedding similarity (cosine): 1.000000
2025-03-30 19:25:32,454 - __main__ - ERROR - CRITICAL (Dynamic): Embeddings are identical! Dynamic LoRA is not being applied during inference.
2025-03-30 19:25:32,454 - __main__ - ERROR - CRITICAL (Merged): Embeddings after merging are identical! LoRA weights are ineffective or merge failed.
2025-03-30 19:25:32,457 - __main__ - INFO - Evaluating 5 trial pairs using wavlm_base_plus
2025-03-30 19:25:32,457 - pretrained_eval - INFO - Using device for model loading: cuda
2025-03-30 19:25:32,457 - pretrained_eval - INFO - CUDA is available. Device count: 1
2025-03-30 19:25:32,457 - pretrained_eval - INFO - Loading pretrained model: microsoft/wavlm-base-plus
2025-03-30 19:25:34,031 - pretrained_eval - INFO - Model moved to cuda
2025-03-30 19:25:34,103 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,104 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,105 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,106 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,107 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.k_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,108 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.v_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,109 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.q_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,109 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.out_proj.lora_B.default.weight with random values
2025-03-30 19:25:34,112 - finetune - INFO - Trainable params: 1179648 || All params: 95638384 || Trainable%: 1.2334%
2025-03-30 19:25:34,689 - __main__ - INFO - Trial pair 1:
2025-03-30 19:25:34,689 - __main__ - INFO -   Label: 1
2025-03-30 19:25:34,689 - __main__ - INFO -   Pretrained score: 0.919183
2025-03-30 19:25:34,689 - __main__ - INFO -   Finetuned score: 0.919183
2025-03-30 19:25:34,689 - __main__ - INFO -   Difference: 0.000000
2025-03-30 19:25:34,820 - __main__ - INFO - Trial pair 2:
2025-03-30 19:25:34,820 - __main__ - INFO -   Label: 0
2025-03-30 19:25:34,820 - __main__ - INFO -   Pretrained score: 0.859243
2025-03-30 19:25:34,820 - __main__ - INFO -   Finetuned score: 0.859243
2025-03-30 19:25:34,820 - __main__ - INFO -   Difference: 0.000000
2025-03-30 19:25:35,033 - __main__ - INFO - Trial pair 3:
2025-03-30 19:25:35,034 - __main__ - INFO -   Label: 1
2025-03-30 19:25:35,034 - __main__ - INFO -   Pretrained score: 0.933051
2025-03-30 19:25:35,034 - __main__ - INFO -   Finetuned score: 0.933051
2025-03-30 19:25:35,034 - __main__ - INFO -   Difference: 0.000000
2025-03-30 19:25:35,149 - __main__ - INFO - Trial pair 4:
2025-03-30 19:25:35,150 - __main__ - INFO -   Label: 0
2025-03-30 19:25:35,150 - __main__ - INFO -   Pretrained score: 0.906886
2025-03-30 19:25:35,150 - __main__ - INFO -   Finetuned score: 0.906886
2025-03-30 19:25:35,150 - __main__ - INFO -   Difference: 0.000000
2025-03-30 19:25:35,277 - __main__ - INFO - Trial pair 5:
2025-03-30 19:25:35,277 - __main__ - INFO -   Label: 1
2025-03-30 19:25:35,277 - __main__ - INFO -   Pretrained score: 0.891734
2025-03-30 19:25:35,277 - __main__ - INFO -   Finetuned score: 0.891734
2025-03-30 19:25:35,277 - __main__ - INFO -   Difference: 0.000000
2025-03-30 19:25:35,278 - __main__ - INFO - Average score difference: 0.000000
2025-03-30 19:25:35,278 - __main__ - ERROR - CRITICAL: Scores are identical! LoRA weights are not being applied.
2025-03-30 19:25:35,281 - __main__ - INFO - --- FINAL DIAGNOSIS ---
2025-03-30 19:25:35,281 - __main__ - ERROR - DIAGNOSIS: LoRA weights were likely trained but are mathematically ineffective OR the merge process failed.
2025-03-30 19:25:35,281 - __main__ - ERROR - Check training loss curve. If loss decreased, the issue might be subtle (e.g., numerical precision, merge bug). If loss didn't decrease, weights weren't trained effectively.
2025-03-30 19:25:35,281 - __main__ - INFO - -----------------------
