2025-03-30 20:16:35,023 - utils - INFO - Using device: cuda
2025-03-30 20:16:35,041 - pretrained_eval - INFO - Initial pretrained_eval device setting: cuda
2025-03-30 20:16:35,652 - speaker_verification.pretrained_eval - INFO - Initial pretrained_eval device setting: cuda
2025-03-30 20:16:35,659 - finetune - INFO - Using device: cuda
2025-03-30 20:16:35,659 - __main__ - INFO - Using device: cuda
2025-03-30 20:16:35,661 - __main__ - INFO - DIAGNOSTIC: Examining saved model structure
2025-03-30 20:16:35,661 - __main__ - INFO - Examining saved model at models/speaker_verification/wavlm_ft/best_model.pt
2025-03-30 20:16:36,142 - __main__ - INFO - State dict contains 345 keys
2025-03-30 20:16:36,142 - __main__ - INFO - Found 96 keys containing 'lora'
2025-03-30 20:16:36,143 - __main__ - INFO - Found 0 keys containing 'adapter'
2025-03-30 20:16:36,252 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.963989
2025-03-30 20:16:36,421 - __main__ - INFO -   Sample values: tensor([-0.0420, -0.0612,  0.0597,  0.0617,  0.0643], device='cuda:0')
2025-03-30 20:16:36,421 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.331726
2025-03-30 20:16:36,422 - __main__ - INFO -   Sample values: tensor([ 0.0019,  0.0216, -0.0017,  0.0085, -0.0192], device='cuda:0')
2025-03-30 20:16:36,422 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=623.872070
2025-03-30 20:16:36,423 - __main__ - INFO -   Sample values: tensor([ 0.0047,  0.0089, -0.1029, -0.0154,  0.0982], device='cuda:0')
2025-03-30 20:16:36,423 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.443237
2025-03-30 20:16:36,424 - __main__ - INFO -   Sample values: tensor([ 0.0014, -0.0011, -0.0072,  0.0076,  0.0037], device='cuda:0')
2025-03-30 20:16:36,424 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=603.753601
2025-03-30 20:16:36,425 - __main__ - INFO -   Sample values: tensor([ 0.0028,  0.0403,  0.0305, -0.0472, -0.0567], device='cuda:0')
2025-03-30 20:16:36,425 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.832474
2025-03-30 20:16:36,426 - __main__ - INFO -   Sample values: tensor([-0.0052,  0.0123,  0.0063, -0.0091,  0.0057], device='cuda:0')
2025-03-30 20:16:36,427 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.221863
2025-03-30 20:16:36,427 - __main__ - INFO -   Sample values: tensor([-0.0214,  0.0453, -0.0945, -0.0655,  0.0986], device='cuda:0')
2025-03-30 20:16:36,428 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=99.093399
2025-03-30 20:16:36,429 - __main__ - INFO -   Sample values: tensor([-0.0070, -0.0187, -0.0088, -0.0166, -0.0043], device='cuda:0')
2025-03-30 20:16:36,429 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=605.234131
2025-03-30 20:16:36,430 - __main__ - INFO -   Sample values: tensor([-0.1142,  0.0035,  0.0388, -0.0214,  0.0942], device='cuda:0')
2025-03-30 20:16:36,430 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.999046
2025-03-30 20:16:36,431 - __main__ - INFO -   Sample values: tensor([-0.0127, -0.0004, -0.0010,  0.0144, -0.0047], device='cuda:0')
2025-03-30 20:16:36,431 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=616.473022
2025-03-30 20:16:36,432 - __main__ - INFO -   Sample values: tensor([-0.1202, -0.0434, -0.1198, -0.0751, -0.0656], device='cuda:0')
2025-03-30 20:16:36,432 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.868164
2025-03-30 20:16:36,433 - __main__ - INFO -   Sample values: tensor([ 0.0146, -0.0032,  0.0077,  0.0159, -0.0167], device='cuda:0')
2025-03-30 20:16:36,433 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.934387
2025-03-30 20:16:36,434 - __main__ - INFO -   Sample values: tensor([-0.0169,  0.0265, -0.0188,  0.1944,  0.0176], device='cuda:0')
2025-03-30 20:16:36,434 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.569046
2025-03-30 20:16:36,435 - __main__ - INFO -   Sample values: tensor([-0.0172,  0.0028, -0.0089,  0.0013,  0.0018], device='cuda:0')
2025-03-30 20:16:36,435 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=607.878479
2025-03-30 20:16:36,436 - __main__ - INFO -   Sample values: tensor([ 0.0651, -0.0916,  0.0081,  0.0113, -0.0661], device='cuda:0')
2025-03-30 20:16:36,436 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.1.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.837250
2025-03-30 20:16:36,437 - __main__ - INFO -   Sample values: tensor([ 0.0078,  0.0140, -0.0018, -0.0132,  0.0028], device='cuda:0')
2025-03-30 20:16:36,437 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=621.444336
2025-03-30 20:16:36,438 - __main__ - INFO -   Sample values: tensor([-0.0569, -0.0408,  0.0872, -0.0295,  0.0524], device='cuda:0')
2025-03-30 20:16:36,438 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.191345
2025-03-30 20:16:36,439 - __main__ - INFO -   Sample values: tensor([ 0.0082, -0.0045,  0.0079,  0.0010, -0.0223], device='cuda:0')
2025-03-30 20:16:36,439 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=608.727112
2025-03-30 20:16:36,440 - __main__ - INFO -   Sample values: tensor([ 0.0061, -0.0239,  0.0554,  0.0618, -0.0104], device='cuda:0')
2025-03-30 20:16:36,440 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.268990
2025-03-30 20:16:36,441 - __main__ - INFO -   Sample values: tensor([ 0.0099,  0.0165, -0.0011,  0.0109,  0.0062], device='cuda:0')
2025-03-30 20:16:36,441 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=611.346375
2025-03-30 20:16:36,442 - __main__ - INFO -   Sample values: tensor([-0.0061,  0.0498,  0.1108, -0.0599, -0.1058], device='cuda:0')
2025-03-30 20:16:36,442 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.723579
2025-03-30 20:16:36,443 - __main__ - INFO -   Sample values: tensor([-0.0098, -0.0075, -0.0013, -0.0035,  0.0162], device='cuda:0')
2025-03-30 20:16:36,443 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=619.286011
2025-03-30 20:16:36,444 - __main__ - INFO -   Sample values: tensor([ 0.0010, -0.0369, -0.0203,  0.0213, -0.0367], device='cuda:0')
2025-03-30 20:16:36,445 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.2.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.884239
2025-03-30 20:16:36,445 - __main__ - INFO -   Sample values: tensor([ 0.0058,  0.0096,  0.0112, -0.0112, -0.0160], device='cuda:0')
2025-03-30 20:16:36,446 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.187134
2025-03-30 20:16:36,446 - __main__ - INFO -   Sample values: tensor([ 0.0436, -0.0300,  0.1110,  0.0028, -0.0311], device='cuda:0')
2025-03-30 20:16:36,447 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.315018
2025-03-30 20:16:36,447 - __main__ - INFO -   Sample values: tensor([ 0.0123,  0.0053,  0.0023, -0.0040,  0.0130], device='cuda:0')
2025-03-30 20:16:36,448 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=613.011841
2025-03-30 20:16:36,448 - __main__ - INFO -   Sample values: tensor([ 0.0980, -0.1131,  0.0900,  0.0619, -0.0398], device='cuda:0')
2025-03-30 20:16:36,449 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.591400
2025-03-30 20:16:36,449 - __main__ - INFO -   Sample values: tensor([0.0028, 0.0234, 0.0010, 0.0111, 0.0273], device='cuda:0')
2025-03-30 20:16:36,450 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=612.362976
2025-03-30 20:16:36,450 - __main__ - INFO -   Sample values: tensor([ 0.0614, -0.0639,  0.0166, -0.0977, -0.1311], device='cuda:0')
2025-03-30 20:16:36,451 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.271263
2025-03-30 20:16:36,451 - __main__ - INFO -   Sample values: tensor([-0.0163,  0.0132,  0.0153,  0.0127,  0.0163], device='cuda:0')
2025-03-30 20:16:36,452 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=609.551392
2025-03-30 20:16:36,452 - __main__ - INFO -   Sample values: tensor([-0.0089, -0.0993, -0.0545,  0.0936,  0.0778], device='cuda:0')
2025-03-30 20:16:36,453 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.3.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.490570
2025-03-30 20:16:36,453 - __main__ - INFO -   Sample values: tensor([ 0.0064,  0.0050, -0.0237, -0.0008, -0.0211], device='cuda:0')
2025-03-30 20:16:36,454 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=616.124878
2025-03-30 20:16:36,454 - __main__ - INFO -   Sample values: tensor([-0.0656, -0.0481, -0.0314,  0.0538, -0.1005], device='cuda:0')
2025-03-30 20:16:36,455 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=99.246002
2025-03-30 20:16:36,455 - __main__ - INFO -   Sample values: tensor([ 0.0037, -0.0049, -0.0115,  0.0013,  0.0097], device='cuda:0')
2025-03-30 20:16:36,455 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=608.231934
2025-03-30 20:16:36,456 - __main__ - INFO -   Sample values: tensor([ 0.0557, -0.0589,  0.1088, -0.0462,  0.0513], device='cuda:0')
2025-03-30 20:16:36,456 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.854225
2025-03-30 20:16:36,457 - __main__ - INFO -   Sample values: tensor([ 0.0041,  0.0032, -0.0025, -0.0037,  0.0009], device='cuda:0')
2025-03-30 20:16:36,457 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=616.154541
2025-03-30 20:16:36,458 - __main__ - INFO -   Sample values: tensor([-0.0274, -0.0573, -0.0796, -0.0729, -0.0407], device='cuda:0')
2025-03-30 20:16:36,458 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.409409
2025-03-30 20:16:36,459 - __main__ - INFO -   Sample values: tensor([-0.0039,  0.0061, -0.0032,  0.0050,  0.0108], device='cuda:0')
2025-03-30 20:16:36,459 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=606.994629
2025-03-30 20:16:36,460 - __main__ - INFO -   Sample values: tensor([-0.0821,  0.0616,  0.0393,  0.0772,  0.1136], device='cuda:0')
2025-03-30 20:16:36,460 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.4.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.823601
2025-03-30 20:16:36,461 - __main__ - INFO -   Sample values: tensor([ 0.0126, -0.0040, -0.0026,  0.0043,  0.0016], device='cuda:0')
2025-03-30 20:16:36,461 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=615.347595
2025-03-30 20:16:36,462 - __main__ - INFO -   Sample values: tensor([ 0.0055,  0.0561, -0.0858,  0.0690,  0.0611], device='cuda:0')
2025-03-30 20:16:36,462 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.074799
2025-03-30 20:16:36,463 - __main__ - INFO -   Sample values: tensor([-0.0066,  0.0047,  0.0018, -0.0072, -0.0068], device='cuda:0')
2025-03-30 20:16:36,463 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=610.753052
2025-03-30 20:16:36,464 - __main__ - INFO -   Sample values: tensor([-0.0612,  0.0924, -0.0281,  0.0142, -0.0552], device='cuda:0')
2025-03-30 20:16:36,464 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.723976
2025-03-30 20:16:36,465 - __main__ - INFO -   Sample values: tensor([ 0.0066,  0.0015,  0.0123, -0.0037,  0.0037], device='cuda:0')
2025-03-30 20:16:36,466 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.018188
2025-03-30 20:16:36,467 - __main__ - INFO -   Sample values: tensor([ 0.1217,  0.1250,  0.0640,  0.0494, -0.1098], device='cuda:0')
2025-03-30 20:16:36,467 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.931793
2025-03-30 20:16:36,468 - __main__ - INFO -   Sample values: tensor([-0.0031,  0.0045, -0.0106,  0.0048,  0.0123], device='cuda:0')
2025-03-30 20:16:36,468 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=603.779419
2025-03-30 20:16:36,469 - __main__ - INFO -   Sample values: tensor([ 0.0547, -0.0522,  0.0450, -0.0401, -0.0213], device='cuda:0')
2025-03-30 20:16:36,469 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.5.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=99.182060
2025-03-30 20:16:36,470 - __main__ - INFO -   Sample values: tensor([-0.0097,  0.0016,  0.0034, -0.0044, -0.0075], device='cuda:0')
2025-03-30 20:16:36,470 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=616.132751
2025-03-30 20:16:36,471 - __main__ - INFO -   Sample values: tensor([-0.0361,  0.1326, -0.0735,  0.0869,  0.0174], device='cuda:0')
2025-03-30 20:16:36,471 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=99.571152
2025-03-30 20:16:36,472 - __main__ - INFO -   Sample values: tensor([-0.0166, -0.0172,  0.0074, -0.0048,  0.0073], device='cuda:0')
2025-03-30 20:16:36,472 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=619.334656
2025-03-30 20:16:36,473 - __main__ - INFO -   Sample values: tensor([-0.0510,  0.0438, -0.0642,  0.0139,  0.0150], device='cuda:0')
2025-03-30 20:16:36,473 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.621506
2025-03-30 20:16:36,474 - __main__ - INFO -   Sample values: tensor([ 0.0044, -0.0109, -0.0068, -0.0032,  0.0016], device='cuda:0')
2025-03-30 20:16:36,474 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.935669
2025-03-30 20:16:36,475 - __main__ - INFO -   Sample values: tensor([ 0.0493, -0.0745,  0.0210, -0.0673, -0.0395], device='cuda:0')
2025-03-30 20:16:36,475 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.452156
2025-03-30 20:16:36,476 - __main__ - INFO -   Sample values: tensor([ 0.0016, -0.0046, -0.0056, -0.0105,  0.0041], device='cuda:0')
2025-03-30 20:16:36,476 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=600.715881
2025-03-30 20:16:36,477 - __main__ - INFO -   Sample values: tensor([-0.0109,  0.0173, -0.0448, -0.0054, -0.0667], device='cuda:0')
2025-03-30 20:16:36,477 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.6.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.149498
2025-03-30 20:16:36,478 - __main__ - INFO -   Sample values: tensor([ 0.0057,  0.0009, -0.0150,  0.0118, -0.0003], device='cuda:0')
2025-03-30 20:16:36,478 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=601.123474
2025-03-30 20:16:36,479 - __main__ - INFO -   Sample values: tensor([-0.0584,  0.0411, -0.0670,  0.0077, -0.0769], device='cuda:0')
2025-03-30 20:16:36,479 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.990265
2025-03-30 20:16:36,480 - __main__ - INFO -   Sample values: tensor([-0.0120,  0.0020, -0.0010,  0.0096,  0.0113], device='cuda:0')
2025-03-30 20:16:36,481 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=611.658447
2025-03-30 20:16:36,481 - __main__ - INFO -   Sample values: tensor([ 0.0239, -0.1329, -0.0470, -0.0025,  0.0168], device='cuda:0')
2025-03-30 20:16:36,482 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.808388
2025-03-30 20:16:36,483 - __main__ - INFO -   Sample values: tensor([-0.0025,  0.0002, -0.0127, -0.0047, -0.0160], device='cuda:0')
2025-03-30 20:16:36,483 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=615.568237
2025-03-30 20:16:36,484 - __main__ - INFO -   Sample values: tensor([-0.0317, -0.0187, -0.0182,  0.1164,  0.1399], device='cuda:0')
2025-03-30 20:16:36,484 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.489014
2025-03-30 20:16:36,485 - __main__ - INFO -   Sample values: tensor([-0.0055, -0.0132,  0.0253, -0.0083, -0.0098], device='cuda:0')
2025-03-30 20:16:36,485 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=611.662598
2025-03-30 20:16:36,486 - __main__ - INFO -   Sample values: tensor([-0.0525, -0.0092, -0.0311,  0.0093,  0.1065], device='cuda:0')
2025-03-30 20:16:36,486 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.7.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.494217
2025-03-30 20:16:36,487 - __main__ - INFO -   Sample values: tensor([ 0.0067, -0.0041,  0.0031, -0.0127,  0.0059], device='cuda:0')
2025-03-30 20:16:36,487 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=619.899475
2025-03-30 20:16:36,488 - __main__ - INFO -   Sample values: tensor([ 0.0822, -0.1088, -0.0537,  0.0115, -0.0984], device='cuda:0')
2025-03-30 20:16:36,488 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.210098
2025-03-30 20:16:36,489 - __main__ - INFO -   Sample values: tensor([ 0.0096, -0.0080, -0.0077,  0.0042,  0.0132], device='cuda:0')
2025-03-30 20:16:36,489 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.577271
2025-03-30 20:16:36,490 - __main__ - INFO -   Sample values: tensor([ 0.0379,  0.0329, -0.0628,  0.1299, -0.0700], device='cuda:0')
2025-03-30 20:16:36,490 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.361526
2025-03-30 20:16:36,491 - __main__ - INFO -   Sample values: tensor([ 0.0010, -0.0107, -0.0002, -0.0151,  0.0002], device='cuda:0')
2025-03-30 20:16:36,491 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.219543
2025-03-30 20:16:36,492 - __main__ - INFO -   Sample values: tensor([ 0.0403, -0.0766, -0.0829,  0.0950,  0.0327], device='cuda:0')
2025-03-30 20:16:36,492 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=99.077545
2025-03-30 20:16:36,493 - __main__ - INFO -   Sample values: tensor([ 0.0030,  0.0025, -0.0167, -0.0160, -0.0028], device='cuda:0')
2025-03-30 20:16:36,493 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=610.664673
2025-03-30 20:16:36,494 - __main__ - INFO -   Sample values: tensor([ 0.0885, -0.0656,  0.0149,  0.0037, -0.0848], device='cuda:0')
2025-03-30 20:16:36,494 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.8.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.257095
2025-03-30 20:16:36,495 - __main__ - INFO -   Sample values: tensor([ 0.0015, -0.0045, -0.0035, -0.0027, -0.0061], device='cuda:0')
2025-03-30 20:16:36,495 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=608.937744
2025-03-30 20:16:36,496 - __main__ - INFO -   Sample values: tensor([-0.0325, -0.1244,  0.0514,  0.0209,  0.0648], device='cuda:0')
2025-03-30 20:16:36,496 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.348587
2025-03-30 20:16:36,497 - __main__ - INFO -   Sample values: tensor([ 0.0073, -0.0048,  0.0015,  0.0197, -0.0022], device='cuda:0')
2025-03-30 20:16:36,497 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=611.153687
2025-03-30 20:16:36,498 - __main__ - INFO -   Sample values: tensor([-0.0101,  0.0604,  0.0553,  0.1337,  0.0503], device='cuda:0')
2025-03-30 20:16:36,498 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.108276
2025-03-30 20:16:36,499 - __main__ - INFO -   Sample values: tensor([-0.0129,  0.0088, -0.0091,  0.0127,  0.0023], device='cuda:0')
2025-03-30 20:16:36,499 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=608.220764
2025-03-30 20:16:36,500 - __main__ - INFO -   Sample values: tensor([ 2.7336e-02, -3.4168e-02,  6.7326e-02,  1.5108e-02,  3.3569e-05],
       device='cuda:0')
2025-03-30 20:16:36,500 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.221489
2025-03-30 20:16:36,501 - __main__ - INFO -   Sample values: tensor([-0.0084,  0.0102, -0.0003,  0.0065,  0.0069], device='cuda:0')
2025-03-30 20:16:36,501 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=609.919006
2025-03-30 20:16:36,502 - __main__ - INFO -   Sample values: tensor([ 0.0675,  0.1030,  0.2009,  0.0171, -0.0295], device='cuda:0')
2025-03-30 20:16:36,502 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.9.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.100441
2025-03-30 20:16:36,503 - __main__ - INFO -   Sample values: tensor([ 0.0008, -0.0034,  0.0051, -0.0106, -0.0069], device='cuda:0')
2025-03-30 20:16:36,503 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=614.026733
2025-03-30 20:16:36,504 - __main__ - INFO -   Sample values: tensor([-0.0270, -0.0730, -0.0067,  0.0015,  0.1003], device='cuda:0')
2025-03-30 20:16:36,505 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.370598
2025-03-30 20:16:36,505 - __main__ - INFO -   Sample values: tensor([-0.0171,  0.0221, -0.0043,  0.0188,  0.0006], device='cuda:0')
2025-03-30 20:16:36,506 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=609.870361
2025-03-30 20:16:36,506 - __main__ - INFO -   Sample values: tensor([-0.1286, -0.0081, -0.0223, -0.0983, -0.0895], device='cuda:0')
2025-03-30 20:16:36,507 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=97.464523
2025-03-30 20:16:36,508 - __main__ - INFO -   Sample values: tensor([-0.0205, -0.0040, -0.0024,  0.0063,  0.0097], device='cuda:0')
2025-03-30 20:16:36,508 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=610.019836
2025-03-30 20:16:36,509 - __main__ - INFO -   Sample values: tensor([ 0.0307, -0.0057,  0.1519,  0.0334,  0.0213], device='cuda:0')
2025-03-30 20:16:36,509 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.215393
2025-03-30 20:16:36,510 - __main__ - INFO -   Sample values: tensor([-0.0181, -0.0119, -0.0130,  0.0022,  0.0037], device='cuda:0')
2025-03-30 20:16:36,510 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=610.744507
2025-03-30 20:16:36,511 - __main__ - INFO -   Sample values: tensor([ 0.0560,  0.0193, -0.0675, -0.0537,  0.0050], device='cuda:0')
2025-03-30 20:16:36,511 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.10.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.652695
2025-03-30 20:16:36,512 - __main__ - INFO -   Sample values: tensor([-0.0023,  0.0049, -0.0094,  0.0019,  0.0052], device='cuda:0')
2025-03-30 20:16:36,512 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=618.005981
2025-03-30 20:16:36,513 - __main__ - INFO -   Sample values: tensor([-0.0806,  0.0424, -0.0328,  0.0019,  0.0011], device='cuda:0')
2025-03-30 20:16:36,513 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.183670
2025-03-30 20:16:36,514 - __main__ - INFO -   Sample values: tensor([-0.0074,  0.0142,  0.0076, -0.0093,  0.0075], device='cuda:0')
2025-03-30 20:16:36,514 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=606.595947
2025-03-30 20:16:36,515 - __main__ - INFO -   Sample values: tensor([ 0.1336, -0.0696, -0.1792,  0.0301,  0.0340], device='cuda:0')
2025-03-30 20:16:36,515 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.532127
2025-03-30 20:16:36,516 - __main__ - INFO -   Sample values: tensor([-0.0138,  0.0153, -0.0002,  0.0019, -0.0119], device='cuda:0')
2025-03-30 20:16:36,516 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=612.319580
2025-03-30 20:16:36,517 - __main__ - INFO -   Sample values: tensor([-0.0186, -0.1323,  0.0500, -0.0555,  0.1024], device='cuda:0')
2025-03-30 20:16:36,517 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.q_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=98.779770
2025-03-30 20:16:36,518 - __main__ - INFO -   Sample values: tensor([ 0.0132,  0.0016,  0.0196, -0.0106,  0.0070], device='cuda:0')
2025-03-30 20:16:36,518 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.out_proj.lora_A.default.weight, shape=torch.Size([16, 768]), sum=617.502502
2025-03-30 20:16:36,519 - __main__ - INFO -   Sample values: tensor([ 0.0106,  0.0149, -0.0402,  0.0037,  0.0518], device='cuda:0')
2025-03-30 20:16:36,519 - __main__ - INFO - Non-zero LoRA parameter: base_model.model.backbone.encoder.layers.11.attention.out_proj.lora_B.default.weight, shape=torch.Size([768, 16]), sum=96.999672
2025-03-30 20:16:36,520 - __main__ - INFO -   Sample values: tensor([ 0.0088,  0.0184, -0.0116,  0.0002, -0.0020], device='cuda:0')
2025-03-30 20:16:36,520 - __main__ - INFO - Summary: Found 96 non-zero and 0 zero/near-zero LoRA/Adapter parameters
2025-03-30 20:16:36,520 - __main__ - INFO - Found 1 ArcFace keys, suggesting this is a complete SpeakerVerificationModel
2025-03-30 20:16:36,520 - __main__ - INFO - ArcFace parameter: base_model.model.arcface.weight, shape=torch.Size([100, 768]), sum=3193.969482
2025-03-30 20:16:36,521 - __main__ - INFO - Loading pretrained results from pretrained_results.csv
2025-03-30 20:16:36,532 - __main__ - INFO - Loaded pretrained results: EER=36.724996681249166%
2025-03-30 20:16:36,532 - __main__ - INFO - Loading pretrained model architecture for wavlm_base_plus
2025-03-30 20:16:36,532 - pretrained_eval - INFO - Using device for model loading: cuda
2025-03-30 20:16:36,532 - pretrained_eval - INFO - CUDA is available. Device count: 1
2025-03-30 20:16:36,532 - pretrained_eval - INFO - Loading pretrained model: microsoft/wavlm-base-plus
2025-03-30 20:16:39,515 - pretrained_eval - INFO - Model moved to cuda
2025-03-30 20:16:39,516 - __main__ - INFO - DIAGNOSTIC: Testing pretrained model forward pass
2025-03-30 20:16:40,346 - __main__ - INFO - Pretrained model outputs last_hidden_state with shape: torch.Size([1, 49, 768])
2025-03-30 20:16:40,346 - __main__ - INFO - Setting up finetuned model architecture for wavlm_base_plus
2025-03-30 20:16:40,347 - __main__ - INFO - Creating SpeakerVerificationModel with backbone and LoRA
2025-03-30 20:16:40,353 - __main__ - INFO - DIAGNOSTIC: Model structure before applying LoRA
2025-03-30 20:16:40,353 - __main__ - INFO - Model has backbone: True
2025-03-30 20:16:40,353 - __main__ - INFO - Model has arcface: True
2025-03-30 20:16:40,353 - __main__ - INFO - Applying LoRA configuration to model
2025-03-30 20:16:40,422 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,422 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,422 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,422 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,422 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.1.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.2.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,423 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.3.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.4.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,424 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.5.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.6.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,425 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.7.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.8.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.9.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,426 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.10.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.k_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.v_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.q_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,427 - finetune - INFO - Explicitly initialized base_model.model.backbone.encoder.layers.11.attention.out_proj.lora_B.default.weight with random values
2025-03-30 20:16:40,429 - finetune - INFO - Trainable params: 1179648 || All params: 95638384 || Trainable%: 1.2334%
2025-03-30 20:16:40,429 - __main__ - INFO - DIAGNOSTIC: Model structure after applying LoRA
2025-03-30 20:16:40,429 - __main__ - INFO - Model has base_model: True
2025-03-30 20:16:40,430 - __main__ - INFO - Model has active_adapter: True
2025-03-30 20:16:40,430 - __main__ - INFO - LoRA parameter found: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight, shape=torch.Size([16, 768])
2025-03-30 20:16:40,430 - __main__ - INFO - LoRA parameter found: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight, shape=torch.Size([768, 16])
2025-03-30 20:16:40,430 - __main__ - INFO - LoRA parameter found: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_A.default.weight, shape=torch.Size([16, 768])
2025-03-30 20:16:40,430 - __main__ - INFO - LoRA parameter found: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight, shape=torch.Size([768, 16])
2025-03-30 20:16:40,430 - __main__ - INFO - LoRA parameter found: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_A.default.weight, shape=torch.Size([16, 768])
2025-03-30 20:16:40,431 - __main__ - INFO - Total LoRA parameters found: 96
2025-03-30 20:16:40,431 - __main__ - WARNING - No enable_adapters method found - adapters should be enabled by default
2025-03-30 20:16:40,431 - __main__ - INFO - Verifying weight loading from models/speaker_verification/wavlm_ft/best_model.pt
2025-03-30 20:16:40,431 - __main__ - INFO - Loading weights from models/speaker_verification/wavlm_ft/best_model.pt
2025-03-30 20:16:40,821 - __main__ - INFO - Keys in saved model containing 'lora':
2025-03-30 20:16:40,821 - __main__ - INFO -   0: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight: shape=torch.Size([16, 768]), sum=-1.2377386093139648
2025-03-30 20:16:40,821 - __main__ - INFO -   1: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight: shape=torch.Size([768, 16]), sum=-0.36364805698394775
2025-03-30 20:16:40,821 - __main__ - INFO -   2: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_A.default.weight: shape=torch.Size([16, 768]), sum=-0.39427804946899414
2025-03-30 20:16:40,821 - __main__ - INFO -   3: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight: shape=torch.Size([768, 16]), sum=0.0002865791320800781
2025-03-30 20:16:40,821 - __main__ - INFO -   4: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_A.default.weight: shape=torch.Size([16, 768]), sum=-6.10112190246582
2025-03-30 20:16:40,821 - __main__ - INFO -   5: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight: shape=torch.Size([768, 16]), sum=0.8208094239234924
2025-03-30 20:16:40,821 - __main__ - INFO -   6: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_A.default.weight: shape=torch.Size([16, 768]), sum=7.329007625579834
2025-03-30 20:16:40,821 - __main__ - INFO -   7: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight: shape=torch.Size([768, 16]), sum=-1.3236995935440063
2025-03-30 20:16:40,821 - __main__ - INFO -   8: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_A.default.weight: shape=torch.Size([16, 768]), sum=-0.1564350128173828
2025-03-30 20:16:40,822 - __main__ - INFO -   9: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight: shape=torch.Size([768, 16]), sum=-1.289433479309082
2025-03-30 20:16:40,824 - __main__ - INFO - Keys in model containing 'lora':
2025-03-30 20:16:40,824 - __main__ - INFO -   0: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight: shape=torch.Size([16, 768])
2025-03-30 20:16:40,824 - __main__ - INFO -   1: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_B.default.weight: shape=torch.Size([768, 16])
2025-03-30 20:16:40,824 - __main__ - INFO -   2: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_A.default.weight: shape=torch.Size([16, 768])
2025-03-30 20:16:40,824 - __main__ - INFO -   3: base_model.model.backbone.encoder.layers.0.attention.v_proj.lora_B.default.weight: shape=torch.Size([768, 16])
2025-03-30 20:16:40,824 - __main__ - INFO -   4: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_A.default.weight: shape=torch.Size([16, 768])
2025-03-30 20:16:40,824 - __main__ - INFO -   5: base_model.model.backbone.encoder.layers.0.attention.q_proj.lora_B.default.weight: shape=torch.Size([768, 16])
2025-03-30 20:16:40,824 - __main__ - INFO -   6: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_A.default.weight: shape=torch.Size([16, 768])
2025-03-30 20:16:40,824 - __main__ - INFO -   7: base_model.model.backbone.encoder.layers.0.attention.out_proj.lora_B.default.weight: shape=torch.Size([768, 16])
2025-03-30 20:16:40,825 - __main__ - INFO -   8: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_A.default.weight: shape=torch.Size([16, 768])
2025-03-30 20:16:40,825 - __main__ - INFO -   9: base_model.model.backbone.encoder.layers.1.attention.k_proj.lora_B.default.weight: shape=torch.Size([768, 16])
2025-03-30 20:16:40,825 - __main__ - INFO - Found non-zero LoRA parameter in saved weights: base_model.model.backbone.encoder.layers.0.attention.k_proj.lora_A.default.weight
2025-03-30 20:16:40,826 - __main__ - INFO -   Sample values: tensor([-0.0420, -0.0612,  0.0597,  0.0617,  0.0643], device='cuda:0')
2025-03-30 20:16:40,868 - __main__ - INFO -   Norm: 6.937424659729004
2025-03-30 20:16:40,869 - __main__ - INFO - Weights successfully changed after loading.
2025-03-30 20:16:40,869 - __main__ - INFO - DIAGNOSTIC: Testing finetuned model forward pass
2025-03-30 20:16:40,896 - __main__ - INFO - Finetuned backbone outputs last_hidden_state with shape: torch.Size([1, 49, 768])
2025-03-30 20:16:40,935 - __main__ - INFO - Finetuned model output type: <class 'torch.Tensor'>
2025-03-30 20:16:40,935 - __main__ - INFO - Using saved subset for faster evaluation: data/vox1/trial_subset_4000.txt
2025-03-30 20:16:40,935 - __main__ - INFO - Evaluating finetuned model performance
2025-03-30 20:16:40,936 - __main__ - INFO - Evaluating Finetuned wavlm_base_plus on data/vox1/trial_subset_4000.txt
2025-03-30 20:16:40,943 - __main__ - INFO - Model type: Finetuned
2025-03-30 20:16:40,979 - __main__ - INFO - Created dataset with 4000 trial pairs
2025-03-30 20:16:40,979 - __main__ - INFO - Processing trial pairs...
Evaluating Finetuned wavlm_base_plus:   0%|          | 0/1000 [00:00<?, ?it/s]Evaluating Finetuned wavlm_base_plus:   0%|          | 1/1000 [00:00<07:47,  2.14it/s]Evaluating Finetuned wavlm_base_plus:   0%|          | 2/1000 [00:00<06:11,  2.69it/s]Evaluating Finetuned wavlm_base_plus:   0%|          | 3/1000 [00:01<05:32,  3.00it/s]Evaluating Finetuned wavlm_base_plus:   0%|          | 4/1000 [00:01<05:41,  2.92it/s]Evaluating Finetuned wavlm_base_plus:   0%|          | 5/1000 [00:01<05:26,  3.05it/s]Evaluating Finetuned wavlm_base_plus:   1%|          | 6/1000 [00:02<05:33,  2.98it/s]Evaluating Finetuned wavlm_base_plus:   1%|          | 7/1000 [00:02<06:37,  2.50it/s]Evaluating Finetuned wavlm_base_plus:   1%|          | 8/1000 [00:03<07:37,  2.17it/s]Evaluating Finetuned wavlm_base_plus:   1%|          | 9/1000 [00:03<07:22,  2.24it/s]Evaluating Finetuned wavlm_base_plus:   1%|1         | 10/1000 [00:03<06:42,  2.46it/s]Evaluating Finetuned wavlm_base_plus:   1%|1         | 11/1000 [00:04<06:42,  2.46it/s]Evaluating Finetuned wavlm_base_plus:   1%|1         | 12/1000 [00:04<06:46,  2.43it/s]Evaluating Finetuned wavlm_base_plus:   1%|1         | 13/1000 [00:05<07:10,  2.29it/s]Evaluating Finetuned wavlm_base_plus:   1%|1         | 14/1000 [00:05<06:30,  2.53it/s]Evaluating Finetuned wavlm_base_plus:   2%|1         | 15/1000 [00:05<05:49,  2.82it/s]Evaluating Finetuned wavlm_base_plus:   2%|1         | 16/1000 [00:06<05:30,  2.98it/s]Evaluating Finetuned wavlm_base_plus:   2%|1         | 17/1000 [00:06<05:25,  3.02it/s]Evaluating Finetuned wavlm_base_plus:   2%|1         | 18/1000 [00:06<05:09,  3.17it/s]Evaluating Finetuned wavlm_base_plus:   2%|1         | 19/1000 [00:07<05:21,  3.05it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 20/1000 [00:07<05:25,  3.01it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 21/1000 [00:07<05:16,  3.09it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 22/1000 [00:07<05:07,  3.18it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 23/1000 [00:08<06:34,  2.48it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 24/1000 [00:09<07:19,  2.22it/s]Evaluating Finetuned wavlm_base_plus:   2%|2         | 25/1000 [00:09<07:53,  2.06it/s]Evaluating Finetuned wavlm_base_plus:   3%|2         | 26/1000 [00:10<07:41,  2.11it/s]Evaluating Finetuned wavlm_base_plus:   3%|2         | 27/1000 [00:10<07:42,  2.10it/s]Evaluating Finetuned wavlm_base_plus:   3%|2         | 28/1000 [00:11<07:05,  2.28it/s]Evaluating Finetuned wavlm_base_plus:   3%|2         | 29/1000 [00:11<06:36,  2.45it/s]Evaluating Finetuned wavlm_base_plus:   3%|3         | 30/1000 [00:11<06:20,  2.55it/s]Evaluating Finetuned wavlm_base_plus:   3%|3         | 31/1000 [00:11<05:38,  2.86it/s]Evaluating Finetuned wavlm_base_plus:   3%|3         | 32/1000 [00:12<05:20,  3.02it/s]Evaluating Finetuned wavlm_base_plus:   3%|3         | 33/1000 [00:12<06:07,  2.63it/s]Evaluating Finetuned wavlm_base_plus:   3%|3         | 34/1000 [00:13<06:03,  2.66it/s]Evaluating Finetuned wavlm_base_plus:   4%|3         | 35/1000 [00:13<06:21,  2.53it/s]Evaluating Finetuned wavlm_base_plus:   4%|3         | 36/1000 [00:13<05:59,  2.68it/s]Evaluating Finetuned wavlm_base_plus:   4%|3         | 37/1000 [00:14<06:23,  2.51it/s]Evaluating Finetuned wavlm_base_plus:   4%|3         | 38/1000 [00:14<06:53,  2.32it/s]Evaluating Finetuned wavlm_base_plus:   4%|3         | 39/1000 [00:15<06:08,  2.61it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 40/1000 [00:15<05:41,  2.81it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 41/1000 [00:15<06:31,  2.45it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 42/1000 [00:16<06:08,  2.60it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 43/1000 [00:16<06:07,  2.61it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 44/1000 [00:17<06:04,  2.63it/s]Evaluating Finetuned wavlm_base_plus:   4%|4         | 45/1000 [00:17<07:32,  2.11it/s]Evaluating Finetuned wavlm_base_plus:   5%|4         | 46/1000 [00:18<08:20,  1.91it/s]Evaluating Finetuned wavlm_base_plus:   5%|4         | 47/1000 [00:18<07:53,  2.01it/s]Evaluating Finetuned wavlm_base_plus:   5%|4         | 48/1000 [00:19<07:05,  2.24it/s]Evaluating Finetuned wavlm_base_plus:   5%|4         | 49/1000 [00:20<09:31,  1.66it/s]Evaluating Finetuned wavlm_base_plus:   5%|5         | 50/1000 [00:20<08:44,  1.81it/s]Evaluating Finetuned wavlm_base_plus:   5%|5         | 51/1000 [00:20<08:07,  1.95it/s]Evaluating Finetuned wavlm_base_plus:   5%|5         | 52/1000 [00:21<07:16,  2.17it/s]Evaluating Finetuned wavlm_base_plus:   5%|5         | 53/1000 [00:21<06:52,  2.30it/s]Evaluating Finetuned wavlm_base_plus:   5%|5         | 54/1000 [00:21<06:06,  2.58it/s]Evaluating Finetuned wavlm_base_plus:   6%|5         | 55/1000 [00:22<05:42,  2.76it/s]Evaluating Finetuned wavlm_base_plus:   6%|5         | 56/1000 [00:22<05:30,  2.86it/s]Evaluating Finetuned wavlm_base_plus:   6%|5         | 57/1000 [00:22<05:21,  2.93it/s]Evaluating Finetuned wavlm_base_plus:   6%|5         | 58/1000 [00:23<05:06,  3.07it/s]Evaluating Finetuned wavlm_base_plus:   6%|5         | 59/1000 [00:23<04:50,  3.24it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 60/1000 [00:23<05:08,  3.04it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 61/1000 [00:24<04:59,  3.13it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 62/1000 [00:24<04:57,  3.15it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 63/1000 [00:24<05:50,  2.67it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 64/1000 [00:25<06:08,  2.54it/s]Evaluating Finetuned wavlm_base_plus:   6%|6         | 65/1000 [00:25<06:23,  2.44it/s]Evaluating Finetuned wavlm_base_plus:   7%|6         | 66/1000 [00:26<06:18,  2.47it/s]Evaluating Finetuned wavlm_base_plus:   7%|6         | 67/1000 [00:26<06:02,  2.58it/s]Evaluating Finetuned wavlm_base_plus:   7%|6         | 68/1000 [00:26<05:48,  2.68it/s]Evaluating Finetuned wavlm_base_plus:   7%|6         | 69/1000 [00:27<06:16,  2.47it/s]Evaluating Finetuned wavlm_base_plus:   7%|7         | 70/1000 [00:27<06:48,  2.27it/s]Evaluating Finetuned wavlm_base_plus:   7%|7         | 71/1000 [00:28<06:32,  2.37it/s]Evaluating Finetuned wavlm_base_plus:   7%|7         | 72/1000 [00:28<06:00,  2.58it/s]Evaluating Finetuned wavlm_base_plus:   7%|7         | 73/1000 [00:28<05:28,  2.82it/s]Evaluating Finetuned wavlm_base_plus:   7%|7         | 74/1000 [00:29<05:13,  2.96it/s]Evaluating Finetuned wavlm_base_plus:   8%|7         | 75/1000 [00:29<05:13,  2.95it/s]Evaluating Finetuned wavlm_base_plus:   8%|7         | 76/1000 [00:29<05:07,  3.01it/s]Evaluating Finetuned wavlm_base_plus:   8%|7         | 77/1000 [00:30<05:40,  2.71it/s]Evaluating Finetuned wavlm_base_plus:   8%|7         | 78/1000 [00:30<06:06,  2.52it/s]Evaluating Finetuned wavlm_base_plus:   8%|7         | 79/1000 [00:31<06:18,  2.44it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 80/1000 [00:31<05:47,  2.65it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 81/1000 [00:31<06:09,  2.49it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 82/1000 [00:32<06:36,  2.32it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 83/1000 [00:32<06:53,  2.22it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 84/1000 [00:33<06:41,  2.28it/s]Evaluating Finetuned wavlm_base_plus:   8%|8         | 85/1000 [00:33<06:58,  2.18it/s]Evaluating Finetuned wavlm_base_plus:   9%|8         | 86/1000 [00:34<06:53,  2.21it/s]Evaluating Finetuned wavlm_base_plus:   9%|8         | 87/1000 [00:34<06:30,  2.34it/s]Evaluating Finetuned wavlm_base_plus:   9%|8         | 88/1000 [00:34<06:00,  2.53it/s]Evaluating Finetuned wavlm_base_plus:   9%|8         | 89/1000 [00:35<05:51,  2.59it/s]Evaluating Finetuned wavlm_base_plus:   9%|9         | 90/1000 [00:35<05:52,  2.58it/s]Evaluating Finetuned wavlm_base_plus:   9%|9         | 91/1000 [00:43<38:49,  2.56s/it]Evaluating Finetuned wavlm_base_plus:   9%|9         | 92/1000 [00:43<29:13,  1.93s/it]Evaluating Finetuned wavlm_base_plus:   9%|9         | 93/1000 [00:44<22:15,  1.47s/it]Evaluating Finetuned wavlm_base_plus:   9%|9         | 94/1000 [00:44<17:05,  1.13s/it]Evaluating Finetuned wavlm_base_plus:  10%|9         | 95/1000 [00:44<13:53,  1.09it/s]Evaluating Finetuned wavlm_base_plus:  10%|9         | 96/1000 [00:45<10:45,  1.40it/s]Evaluating Finetuned wavlm_base_plus:  10%|9         | 97/1000 [00:45<08:52,  1.69it/s]Evaluating Finetuned wavlm_base_plus:  10%|9         | 98/1000 [00:45<07:29,  2.00it/s]Evaluating Finetuned wavlm_base_plus:  10%|9         | 99/1000 [00:46<06:34,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 100/1000 [00:46<06:40,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 101/1000 [00:46<05:50,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 102/1000 [00:47<05:15,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 103/1000 [00:47<05:01,  2.98it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 104/1000 [00:47<05:07,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  10%|#         | 105/1000 [00:47<04:44,  3.14it/s]Evaluating Finetuned wavlm_base_plus:  11%|#         | 106/1000 [00:48<05:03,  2.94it/s]Evaluating Finetuned wavlm_base_plus:  11%|#         | 107/1000 [00:48<04:49,  3.08it/s]Evaluating Finetuned wavlm_base_plus:  11%|#         | 108/1000 [00:48<04:38,  3.20it/s]Evaluating Finetuned wavlm_base_plus:  11%|#         | 109/1000 [00:49<05:25,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  11%|#1        | 110/1000 [00:49<05:54,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  11%|#1        | 111/1000 [00:50<05:21,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  11%|#1        | 112/1000 [00:50<05:21,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  11%|#1        | 113/1000 [00:50<05:31,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  11%|#1        | 114/1000 [00:51<05:22,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  12%|#1        | 115/1000 [00:51<05:06,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  12%|#1        | 116/1000 [00:51<04:52,  3.02it/s]Evaluating Finetuned wavlm_base_plus:  12%|#1        | 117/1000 [00:52<04:48,  3.06it/s]Evaluating Finetuned wavlm_base_plus:  12%|#1        | 118/1000 [00:52<05:22,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  12%|#1        | 119/1000 [00:52<04:58,  2.95it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 120/1000 [00:53<04:57,  2.96it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 121/1000 [00:53<05:01,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 122/1000 [00:54<06:00,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 123/1000 [00:54<05:21,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 124/1000 [00:54<04:51,  3.00it/s]Evaluating Finetuned wavlm_base_plus:  12%|#2        | 125/1000 [00:55<04:45,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  13%|#2        | 126/1000 [00:55<04:45,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  13%|#2        | 127/1000 [00:55<04:29,  3.24it/s]Evaluating Finetuned wavlm_base_plus:  13%|#2        | 128/1000 [00:55<04:32,  3.20it/s]Evaluating Finetuned wavlm_base_plus:  13%|#2        | 129/1000 [00:56<04:24,  3.30it/s]Evaluating Finetuned wavlm_base_plus:  13%|#3        | 130/1000 [00:56<04:23,  3.30it/s]Evaluating Finetuned wavlm_base_plus:  13%|#3        | 131/1000 [00:56<04:31,  3.20it/s]Evaluating Finetuned wavlm_base_plus:  13%|#3        | 132/1000 [00:57<04:41,  3.09it/s]Evaluating Finetuned wavlm_base_plus:  13%|#3        | 133/1000 [00:57<04:37,  3.13it/s]Evaluating Finetuned wavlm_base_plus:  13%|#3        | 134/1000 [00:57<04:25,  3.26it/s]Evaluating Finetuned wavlm_base_plus:  14%|#3        | 135/1000 [00:58<04:25,  3.26it/s]Evaluating Finetuned wavlm_base_plus:  14%|#3        | 136/1000 [00:58<04:30,  3.19it/s]Evaluating Finetuned wavlm_base_plus:  14%|#3        | 137/1000 [00:58<04:36,  3.12it/s]Evaluating Finetuned wavlm_base_plus:  14%|#3        | 138/1000 [00:59<04:35,  3.13it/s]Evaluating Finetuned wavlm_base_plus:  14%|#3        | 139/1000 [00:59<04:57,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 140/1000 [00:59<04:49,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 141/1000 [01:00<05:41,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 142/1000 [01:00<05:06,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 143/1000 [01:00<04:57,  2.88it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 144/1000 [01:01<04:42,  3.03it/s]Evaluating Finetuned wavlm_base_plus:  14%|#4        | 145/1000 [01:01<05:03,  2.82it/s]Evaluating Finetuned wavlm_base_plus:  15%|#4        | 146/1000 [01:02<05:35,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  15%|#4        | 147/1000 [01:02<05:53,  2.41it/s]Evaluating Finetuned wavlm_base_plus:  15%|#4        | 148/1000 [01:02<05:39,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  15%|#4        | 149/1000 [01:03<05:26,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  15%|#5        | 150/1000 [01:03<05:51,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  15%|#5        | 151/1000 [01:04<05:17,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  15%|#5        | 152/1000 [01:04<04:56,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  15%|#5        | 153/1000 [01:04<04:39,  3.03it/s]Evaluating Finetuned wavlm_base_plus:  15%|#5        | 154/1000 [01:05<04:45,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  16%|#5        | 155/1000 [01:05<04:44,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  16%|#5        | 156/1000 [01:05<05:07,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  16%|#5        | 157/1000 [01:06<04:56,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  16%|#5        | 158/1000 [01:06<04:35,  3.06it/s]Evaluating Finetuned wavlm_base_plus:  16%|#5        | 159/1000 [01:06<04:35,  3.05it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 160/1000 [01:07<04:34,  3.06it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 161/1000 [01:07<04:41,  2.98it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 162/1000 [01:07<04:39,  3.00it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 163/1000 [01:08<05:04,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 164/1000 [01:08<05:01,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  16%|#6        | 165/1000 [01:08<05:03,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  17%|#6        | 166/1000 [01:09<04:40,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  17%|#6        | 167/1000 [01:09<05:12,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  17%|#6        | 168/1000 [01:10<05:30,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  17%|#6        | 169/1000 [01:10<05:38,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  17%|#7        | 170/1000 [01:10<05:52,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  17%|#7        | 171/1000 [01:11<05:37,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  17%|#7        | 172/1000 [01:11<05:19,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  17%|#7        | 173/1000 [01:12<05:34,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  17%|#7        | 174/1000 [01:12<05:20,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  18%|#7        | 175/1000 [01:12<05:11,  2.65it/s]Evaluating Finetuned wavlm_base_plus:  18%|#7        | 176/1000 [01:13<05:32,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  18%|#7        | 177/1000 [01:13<05:48,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  18%|#7        | 178/1000 [01:14<05:48,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  18%|#7        | 179/1000 [01:14<05:27,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 180/1000 [01:14<05:44,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 181/1000 [01:15<05:18,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 182/1000 [01:15<04:59,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 183/1000 [01:16<05:27,  2.49it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 184/1000 [01:16<05:05,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  18%|#8        | 185/1000 [01:16<05:06,  2.66it/s]Evaluating Finetuned wavlm_base_plus:  19%|#8        | 186/1000 [01:17<04:51,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  19%|#8        | 187/1000 [01:17<04:22,  3.10it/s]Evaluating Finetuned wavlm_base_plus:  19%|#8        | 188/1000 [01:17<04:14,  3.19it/s]Evaluating Finetuned wavlm_base_plus:  19%|#8        | 189/1000 [01:18<05:08,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  19%|#9        | 190/1000 [01:18<04:37,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  19%|#9        | 191/1000 [01:18<04:31,  2.98it/s]Evaluating Finetuned wavlm_base_plus:  19%|#9        | 192/1000 [01:19<04:52,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  19%|#9        | 193/1000 [01:19<04:40,  2.88it/s]Evaluating Finetuned wavlm_base_plus:  19%|#9        | 194/1000 [01:19<04:42,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  20%|#9        | 195/1000 [01:20<04:25,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  20%|#9        | 196/1000 [01:20<04:14,  3.16it/s]Evaluating Finetuned wavlm_base_plus:  20%|#9        | 197/1000 [01:20<05:11,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  20%|#9        | 198/1000 [01:21<06:11,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  20%|#9        | 199/1000 [01:21<05:37,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  20%|##        | 200/1000 [01:22<05:02,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  20%|##        | 201/1000 [01:22<05:02,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  20%|##        | 202/1000 [01:30<34:50,  2.62s/it]Evaluating Finetuned wavlm_base_plus:  20%|##        | 203/1000 [01:30<25:59,  1.96s/it]Evaluating Finetuned wavlm_base_plus:  20%|##        | 204/1000 [01:31<19:48,  1.49s/it]Evaluating Finetuned wavlm_base_plus:  20%|##        | 205/1000 [01:31<15:00,  1.13s/it]Evaluating Finetuned wavlm_base_plus:  21%|##        | 206/1000 [01:31<12:02,  1.10it/s]Evaluating Finetuned wavlm_base_plus:  21%|##        | 207/1000 [01:32<09:30,  1.39it/s]Evaluating Finetuned wavlm_base_plus:  21%|##        | 208/1000 [01:32<07:45,  1.70it/s]Evaluating Finetuned wavlm_base_plus:  21%|##        | 209/1000 [01:32<06:44,  1.95it/s]Evaluating Finetuned wavlm_base_plus:  21%|##1       | 210/1000 [01:33<06:05,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  21%|##1       | 211/1000 [01:33<06:11,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  21%|##1       | 212/1000 [01:34<06:00,  2.18it/s]Evaluating Finetuned wavlm_base_plus:  21%|##1       | 213/1000 [01:34<05:25,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  21%|##1       | 214/1000 [01:34<04:45,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  22%|##1       | 215/1000 [01:35<04:50,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  22%|##1       | 216/1000 [01:35<04:49,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  22%|##1       | 217/1000 [01:35<05:34,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  22%|##1       | 218/1000 [01:36<05:59,  2.18it/s]Evaluating Finetuned wavlm_base_plus:  22%|##1       | 219/1000 [01:36<05:33,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 220/1000 [01:37<05:00,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 221/1000 [01:37<04:57,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 222/1000 [01:37<04:50,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 223/1000 [01:38<04:56,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 224/1000 [01:38<05:05,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  22%|##2       | 225/1000 [01:38<04:43,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  23%|##2       | 226/1000 [01:39<04:21,  2.96it/s]Evaluating Finetuned wavlm_base_plus:  23%|##2       | 227/1000 [01:39<04:24,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  23%|##2       | 228/1000 [01:40<04:40,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  23%|##2       | 229/1000 [01:40<04:32,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  23%|##3       | 230/1000 [01:40<04:11,  3.06it/s]Evaluating Finetuned wavlm_base_plus:  23%|##3       | 231/1000 [01:41<04:28,  2.87it/s]Evaluating Finetuned wavlm_base_plus:  23%|##3       | 232/1000 [01:41<04:33,  2.81it/s]Evaluating Finetuned wavlm_base_plus:  23%|##3       | 233/1000 [01:41<04:12,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  23%|##3       | 234/1000 [01:41<03:55,  3.25it/s]Evaluating Finetuned wavlm_base_plus:  24%|##3       | 235/1000 [01:42<03:49,  3.33it/s]Evaluating Finetuned wavlm_base_plus:  24%|##3       | 236/1000 [01:42<03:52,  3.28it/s]Evaluating Finetuned wavlm_base_plus:  24%|##3       | 237/1000 [01:42<04:10,  3.05it/s]Evaluating Finetuned wavlm_base_plus:  24%|##3       | 238/1000 [01:43<04:19,  2.94it/s]Evaluating Finetuned wavlm_base_plus:  24%|##3       | 239/1000 [01:43<05:35,  2.27it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 240/1000 [01:44<05:10,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 241/1000 [01:44<05:08,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 242/1000 [01:45<05:06,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 243/1000 [01:45<05:35,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 244/1000 [01:46<05:50,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  24%|##4       | 245/1000 [01:46<05:56,  2.12it/s]Evaluating Finetuned wavlm_base_plus:  25%|##4       | 246/1000 [01:47<05:54,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  25%|##4       | 247/1000 [01:47<05:15,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  25%|##4       | 248/1000 [01:47<04:37,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  25%|##4       | 249/1000 [01:47<04:34,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  25%|##5       | 250/1000 [01:48<04:32,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  25%|##5       | 251/1000 [01:48<04:22,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  25%|##5       | 252/1000 [01:49<04:27,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  25%|##5       | 253/1000 [01:49<04:25,  2.82it/s]Evaluating Finetuned wavlm_base_plus:  25%|##5       | 254/1000 [01:49<04:19,  2.88it/s]Evaluating Finetuned wavlm_base_plus:  26%|##5       | 255/1000 [01:50<04:49,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  26%|##5       | 256/1000 [01:50<04:38,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  26%|##5       | 257/1000 [01:50<04:28,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  26%|##5       | 258/1000 [01:51<04:25,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  26%|##5       | 259/1000 [01:51<04:14,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 260/1000 [01:51<03:58,  3.10it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 261/1000 [01:52<03:59,  3.08it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 262/1000 [01:52<04:08,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 263/1000 [01:52<04:18,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 264/1000 [01:53<04:03,  3.02it/s]Evaluating Finetuned wavlm_base_plus:  26%|##6       | 265/1000 [01:53<03:48,  3.21it/s]Evaluating Finetuned wavlm_base_plus:  27%|##6       | 266/1000 [01:53<03:55,  3.12it/s]Evaluating Finetuned wavlm_base_plus:  27%|##6       | 267/1000 [01:54<04:15,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  27%|##6       | 268/1000 [01:54<04:29,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  27%|##6       | 269/1000 [01:54<04:34,  2.66it/s]Evaluating Finetuned wavlm_base_plus:  27%|##7       | 270/1000 [01:55<04:26,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  27%|##7       | 271/1000 [01:55<04:24,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  27%|##7       | 272/1000 [01:56<04:16,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  27%|##7       | 273/1000 [01:56<04:05,  2.96it/s]Evaluating Finetuned wavlm_base_plus:  27%|##7       | 274/1000 [01:56<03:52,  3.12it/s]Evaluating Finetuned wavlm_base_plus:  28%|##7       | 275/1000 [01:56<03:51,  3.13it/s]Evaluating Finetuned wavlm_base_plus:  28%|##7       | 276/1000 [01:57<03:43,  3.24it/s]Evaluating Finetuned wavlm_base_plus:  28%|##7       | 277/1000 [01:57<03:58,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  28%|##7       | 278/1000 [01:57<03:52,  3.10it/s]Evaluating Finetuned wavlm_base_plus:  28%|##7       | 279/1000 [01:58<04:33,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 280/1000 [01:58<04:13,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 281/1000 [01:59<04:42,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 282/1000 [01:59<05:26,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 283/1000 [02:00<05:32,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 284/1000 [02:00<05:05,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  28%|##8       | 285/1000 [02:01<05:04,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  29%|##8       | 286/1000 [02:01<04:56,  2.41it/s]Evaluating Finetuned wavlm_base_plus:  29%|##8       | 287/1000 [02:01<04:30,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  29%|##8       | 288/1000 [02:02<04:20,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  29%|##8       | 289/1000 [02:02<05:09,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  29%|##9       | 290/1000 [02:03<05:08,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  29%|##9       | 291/1000 [02:03<04:50,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  29%|##9       | 292/1000 [02:03<04:34,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  29%|##9       | 293/1000 [02:04<04:28,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  29%|##9       | 294/1000 [02:04<05:06,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  30%|##9       | 295/1000 [02:04<04:41,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  30%|##9       | 296/1000 [02:05<04:27,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  30%|##9       | 297/1000 [02:05<04:29,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  30%|##9       | 298/1000 [02:06<04:32,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  30%|##9       | 299/1000 [02:06<04:03,  2.88it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 300/1000 [02:06<04:11,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 301/1000 [02:07<04:59,  2.33it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 302/1000 [02:07<05:41,  2.05it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 303/1000 [02:08<05:35,  2.08it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 304/1000 [02:08<05:06,  2.27it/s]Evaluating Finetuned wavlm_base_plus:  30%|###       | 305/1000 [02:09<04:46,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  31%|###       | 306/1000 [02:09<04:34,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  31%|###       | 307/1000 [02:09<04:22,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  31%|###       | 308/1000 [02:10<04:10,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  31%|###       | 309/1000 [02:10<04:35,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  31%|###1      | 310/1000 [02:10<04:23,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  31%|###1      | 311/1000 [02:11<04:13,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  31%|###1      | 312/1000 [02:11<05:01,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  31%|###1      | 313/1000 [02:12<04:40,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  31%|###1      | 314/1000 [02:12<04:13,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  32%|###1      | 315/1000 [02:12<04:21,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  32%|###1      | 316/1000 [02:13<04:36,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  32%|###1      | 317/1000 [02:13<04:45,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  32%|###1      | 318/1000 [02:14<04:56,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  32%|###1      | 319/1000 [02:14<05:02,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 320/1000 [02:15<04:24,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 321/1000 [02:15<04:37,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 322/1000 [02:15<04:32,  2.49it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 323/1000 [02:16<04:09,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 324/1000 [02:16<04:02,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  32%|###2      | 325/1000 [02:16<03:45,  3.00it/s]Evaluating Finetuned wavlm_base_plus:  33%|###2      | 326/1000 [02:17<03:43,  3.02it/s]Evaluating Finetuned wavlm_base_plus:  33%|###2      | 327/1000 [02:17<03:39,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  33%|###2      | 328/1000 [02:17<03:56,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  33%|###2      | 329/1000 [02:18<04:01,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  33%|###3      | 330/1000 [02:18<03:45,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  33%|###3      | 331/1000 [02:18<03:41,  3.02it/s]Evaluating Finetuned wavlm_base_plus:  33%|###3      | 332/1000 [02:19<03:40,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  33%|###3      | 333/1000 [02:19<04:03,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  33%|###3      | 334/1000 [02:19<03:57,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  34%|###3      | 335/1000 [02:20<03:48,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  34%|###3      | 336/1000 [02:20<03:33,  3.11it/s]Evaluating Finetuned wavlm_base_plus:  34%|###3      | 337/1000 [02:20<03:16,  3.37it/s]Evaluating Finetuned wavlm_base_plus:  34%|###3      | 338/1000 [02:21<03:19,  3.32it/s]Evaluating Finetuned wavlm_base_plus:  34%|###3      | 339/1000 [02:21<03:11,  3.46it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 340/1000 [02:21<03:12,  3.43it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 341/1000 [02:22<03:45,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 342/1000 [02:22<03:43,  2.94it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 343/1000 [02:22<03:35,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 344/1000 [02:23<03:35,  3.04it/s]Evaluating Finetuned wavlm_base_plus:  34%|###4      | 345/1000 [02:23<03:33,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  35%|###4      | 346/1000 [02:23<03:33,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  35%|###4      | 347/1000 [02:24<03:32,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  35%|###4      | 348/1000 [02:24<03:41,  2.94it/s]Evaluating Finetuned wavlm_base_plus:  35%|###4      | 349/1000 [02:24<03:28,  3.13it/s]Evaluating Finetuned wavlm_base_plus:  35%|###5      | 350/1000 [02:24<03:19,  3.26it/s]Evaluating Finetuned wavlm_base_plus:  35%|###5      | 351/1000 [02:25<03:08,  3.44it/s]Evaluating Finetuned wavlm_base_plus:  35%|###5      | 352/1000 [02:25<04:13,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  35%|###5      | 353/1000 [02:26<04:06,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  35%|###5      | 354/1000 [02:26<03:43,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  36%|###5      | 355/1000 [02:26<03:41,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  36%|###5      | 356/1000 [02:27<03:44,  2.87it/s]Evaluating Finetuned wavlm_base_plus:  36%|###5      | 357/1000 [02:27<03:36,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  36%|###5      | 358/1000 [02:27<03:26,  3.12it/s]Evaluating Finetuned wavlm_base_plus:  36%|###5      | 359/1000 [02:28<03:18,  3.24it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 360/1000 [02:28<03:18,  3.22it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 361/1000 [02:28<03:47,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 362/1000 [02:29<04:02,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 363/1000 [02:29<04:09,  2.55it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 364/1000 [02:30<04:03,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  36%|###6      | 365/1000 [02:30<03:44,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  37%|###6      | 366/1000 [02:30<03:38,  2.90it/s]Evaluating Finetuned wavlm_base_plus:  37%|###6      | 367/1000 [02:30<03:33,  2.96it/s]Evaluating Finetuned wavlm_base_plus:  37%|###6      | 368/1000 [02:31<03:34,  2.95it/s]Evaluating Finetuned wavlm_base_plus:  37%|###6      | 369/1000 [02:31<03:35,  2.93it/s]Evaluating Finetuned wavlm_base_plus:  37%|###7      | 370/1000 [02:31<03:37,  2.90it/s]Evaluating Finetuned wavlm_base_plus:  37%|###7      | 371/1000 [02:33<07:22,  1.42it/s]Evaluating Finetuned wavlm_base_plus:  37%|###7      | 372/1000 [02:33<06:27,  1.62it/s]Evaluating Finetuned wavlm_base_plus:  37%|###7      | 373/1000 [02:34<05:25,  1.92it/s]Evaluating Finetuned wavlm_base_plus:  37%|###7      | 374/1000 [02:34<04:49,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  38%|###7      | 375/1000 [02:34<04:14,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  38%|###7      | 376/1000 [02:35<03:51,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  38%|###7      | 377/1000 [02:35<03:49,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  38%|###7      | 378/1000 [02:35<03:26,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  38%|###7      | 379/1000 [02:36<03:36,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 380/1000 [02:36<03:32,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 381/1000 [02:36<03:33,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 382/1000 [02:37<03:39,  2.81it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 383/1000 [02:37<03:20,  3.08it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 384/1000 [02:37<03:12,  3.19it/s]Evaluating Finetuned wavlm_base_plus:  38%|###8      | 385/1000 [02:38<03:24,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  39%|###8      | 386/1000 [02:38<03:23,  3.02it/s]Evaluating Finetuned wavlm_base_plus:  39%|###8      | 387/1000 [02:38<03:23,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  39%|###8      | 388/1000 [02:39<03:27,  2.95it/s]Evaluating Finetuned wavlm_base_plus:  39%|###8      | 389/1000 [02:39<03:41,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  39%|###9      | 390/1000 [02:39<03:27,  2.93it/s]Evaluating Finetuned wavlm_base_plus:  39%|###9      | 391/1000 [02:40<03:21,  3.03it/s]Evaluating Finetuned wavlm_base_plus:  39%|###9      | 392/1000 [02:40<03:34,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  39%|###9      | 393/1000 [02:40<03:42,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  39%|###9      | 394/1000 [02:41<03:41,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  40%|###9      | 395/1000 [02:41<03:31,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  40%|###9      | 396/1000 [02:41<03:25,  2.94it/s]Evaluating Finetuned wavlm_base_plus:  40%|###9      | 397/1000 [02:42<04:04,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  40%|###9      | 398/1000 [02:42<04:09,  2.41it/s]Evaluating Finetuned wavlm_base_plus:  40%|###9      | 399/1000 [02:43<04:07,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 400/1000 [02:43<03:59,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 401/1000 [02:44<03:56,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 402/1000 [02:44<03:55,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 403/1000 [02:44<04:05,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 404/1000 [02:45<04:05,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  40%|####      | 405/1000 [02:45<03:50,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  41%|####      | 406/1000 [02:46<03:51,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  41%|####      | 407/1000 [02:46<04:05,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  41%|####      | 408/1000 [02:47<04:57,  1.99it/s]Evaluating Finetuned wavlm_base_plus:  41%|####      | 409/1000 [02:47<04:15,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  41%|####1     | 410/1000 [02:47<04:12,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  41%|####1     | 411/1000 [02:48<03:53,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  41%|####1     | 412/1000 [02:48<03:40,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  41%|####1     | 413/1000 [02:48<03:35,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  41%|####1     | 414/1000 [02:49<03:27,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  42%|####1     | 415/1000 [02:49<03:14,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  42%|####1     | 416/1000 [02:49<03:03,  3.18it/s]Evaluating Finetuned wavlm_base_plus:  42%|####1     | 417/1000 [02:50<03:09,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  42%|####1     | 418/1000 [02:50<03:19,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  42%|####1     | 419/1000 [02:50<03:27,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 420/1000 [02:51<03:39,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 421/1000 [02:52<04:53,  1.97it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 422/1000 [02:52<05:00,  1.93it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 423/1000 [02:53<04:29,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 424/1000 [02:53<04:00,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  42%|####2     | 425/1000 [02:53<03:46,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  43%|####2     | 426/1000 [02:54<03:36,  2.65it/s]Evaluating Finetuned wavlm_base_plus:  43%|####2     | 427/1000 [02:54<04:46,  2.00it/s]Evaluating Finetuned wavlm_base_plus:  43%|####2     | 428/1000 [02:55<05:29,  1.74it/s]Evaluating Finetuned wavlm_base_plus:  43%|####2     | 429/1000 [02:56<06:10,  1.54it/s]Evaluating Finetuned wavlm_base_plus:  43%|####3     | 430/1000 [02:56<05:23,  1.76it/s]Evaluating Finetuned wavlm_base_plus:  43%|####3     | 431/1000 [02:57<04:52,  1.95it/s]Evaluating Finetuned wavlm_base_plus:  43%|####3     | 432/1000 [02:57<04:26,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  43%|####3     | 433/1000 [02:57<04:07,  2.29it/s]Evaluating Finetuned wavlm_base_plus:  43%|####3     | 434/1000 [02:58<03:51,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  44%|####3     | 435/1000 [02:58<03:30,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  44%|####3     | 436/1000 [02:58<03:30,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  44%|####3     | 437/1000 [02:59<03:56,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  44%|####3     | 438/1000 [02:59<03:37,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  44%|####3     | 439/1000 [03:00<03:35,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 440/1000 [03:00<03:37,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 441/1000 [03:01<04:20,  2.15it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 442/1000 [03:01<05:03,  1.84it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 443/1000 [03:02<04:36,  2.01it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 444/1000 [03:02<04:16,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  44%|####4     | 445/1000 [03:03<04:03,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  45%|####4     | 446/1000 [03:03<03:38,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  45%|####4     | 447/1000 [03:04<06:55,  1.33it/s]Evaluating Finetuned wavlm_base_plus:  45%|####4     | 448/1000 [03:05<05:41,  1.62it/s]Evaluating Finetuned wavlm_base_plus:  45%|####4     | 449/1000 [03:05<04:54,  1.87it/s]Evaluating Finetuned wavlm_base_plus:  45%|####5     | 450/1000 [03:05<04:30,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  45%|####5     | 451/1000 [03:06<04:02,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  45%|####5     | 452/1000 [03:06<04:02,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  45%|####5     | 453/1000 [03:07<04:28,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  45%|####5     | 454/1000 [03:07<04:03,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  46%|####5     | 455/1000 [03:08<04:12,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  46%|####5     | 456/1000 [03:08<04:11,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  46%|####5     | 457/1000 [03:09<05:13,  1.73it/s]Evaluating Finetuned wavlm_base_plus:  46%|####5     | 458/1000 [03:10<05:25,  1.67it/s]Evaluating Finetuned wavlm_base_plus:  46%|####5     | 459/1000 [03:10<04:37,  1.95it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 460/1000 [03:10<04:09,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 461/1000 [03:11<03:44,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 462/1000 [03:11<03:24,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 463/1000 [03:11<03:11,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 464/1000 [03:12<03:11,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  46%|####6     | 465/1000 [03:12<03:52,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  47%|####6     | 466/1000 [03:13<04:10,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  47%|####6     | 467/1000 [03:13<04:17,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  47%|####6     | 468/1000 [03:14<04:16,  2.08it/s]Evaluating Finetuned wavlm_base_plus:  47%|####6     | 469/1000 [03:14<03:43,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  47%|####6     | 470/1000 [03:14<03:29,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  47%|####7     | 471/1000 [03:15<04:06,  2.15it/s]Evaluating Finetuned wavlm_base_plus:  47%|####7     | 472/1000 [03:16<04:38,  1.90it/s]Evaluating Finetuned wavlm_base_plus:  47%|####7     | 473/1000 [03:16<04:42,  1.86it/s]Evaluating Finetuned wavlm_base_plus:  47%|####7     | 474/1000 [03:17<04:27,  1.96it/s]Evaluating Finetuned wavlm_base_plus:  48%|####7     | 475/1000 [03:17<04:12,  2.08it/s]Evaluating Finetuned wavlm_base_plus:  48%|####7     | 476/1000 [03:17<03:49,  2.29it/s]Evaluating Finetuned wavlm_base_plus:  48%|####7     | 477/1000 [03:18<03:24,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  48%|####7     | 478/1000 [03:18<03:23,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  48%|####7     | 479/1000 [03:19<03:36,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 480/1000 [03:19<03:34,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 481/1000 [03:19<03:09,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 482/1000 [03:20<03:15,  2.65it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 483/1000 [03:20<03:10,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 484/1000 [03:20<03:10,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  48%|####8     | 485/1000 [03:21<03:28,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  49%|####8     | 486/1000 [03:21<03:20,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  49%|####8     | 487/1000 [03:22<03:20,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  49%|####8     | 488/1000 [03:22<02:59,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  49%|####8     | 489/1000 [03:22<02:55,  2.91it/s]Evaluating Finetuned wavlm_base_plus:  49%|####9     | 490/1000 [03:23<03:01,  2.81it/s]Evaluating Finetuned wavlm_base_plus:  49%|####9     | 491/1000 [03:23<03:01,  2.80it/s]Evaluating Finetuned wavlm_base_plus:  49%|####9     | 492/1000 [03:23<03:03,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  49%|####9     | 493/1000 [03:24<03:45,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  49%|####9     | 494/1000 [03:25<04:24,  1.91it/s]Evaluating Finetuned wavlm_base_plus:  50%|####9     | 495/1000 [03:25<04:23,  1.92it/s]Evaluating Finetuned wavlm_base_plus:  50%|####9     | 496/1000 [03:26<04:05,  2.06it/s]Evaluating Finetuned wavlm_base_plus:  50%|####9     | 497/1000 [03:26<04:03,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  50%|####9     | 498/1000 [03:26<03:47,  2.21it/s]Evaluating Finetuned wavlm_base_plus:  50%|####9     | 499/1000 [03:27<03:19,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 500/1000 [03:27<03:22,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 501/1000 [03:27<03:20,  2.49it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 502/1000 [03:28<03:13,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 503/1000 [03:28<03:09,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 504/1000 [03:29<03:05,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  50%|#####     | 505/1000 [03:29<02:54,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####     | 506/1000 [03:29<03:10,  2.59it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####     | 507/1000 [03:30<03:01,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####     | 508/1000 [03:30<02:58,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####     | 509/1000 [03:30<03:01,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####1    | 510/1000 [03:31<02:49,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####1    | 511/1000 [03:31<02:43,  2.99it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####1    | 512/1000 [03:31<02:34,  3.15it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####1    | 513/1000 [03:32<02:38,  3.07it/s]Evaluating Finetuned wavlm_base_plus:  51%|#####1    | 514/1000 [03:32<02:42,  3.00it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####1    | 515/1000 [03:32<02:41,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####1    | 516/1000 [03:33<02:54,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####1    | 517/1000 [03:33<02:40,  3.01it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####1    | 518/1000 [03:33<02:27,  3.28it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####1    | 519/1000 [03:33<02:19,  3.46it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 520/1000 [03:34<02:18,  3.47it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 521/1000 [03:34<02:57,  2.69it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 522/1000 [03:35<03:23,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 523/1000 [03:35<03:12,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 524/1000 [03:36<03:12,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  52%|#####2    | 525/1000 [03:36<02:56,  2.69it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####2    | 526/1000 [03:36<02:55,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####2    | 527/1000 [03:37<02:49,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####2    | 528/1000 [03:37<02:51,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####2    | 529/1000 [03:37<02:49,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####3    | 530/1000 [03:39<05:00,  1.56it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####3    | 531/1000 [03:39<05:22,  1.45it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####3    | 532/1000 [03:40<05:35,  1.40it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####3    | 533/1000 [03:41<06:11,  1.26it/s]Evaluating Finetuned wavlm_base_plus:  53%|#####3    | 534/1000 [03:42<06:36,  1.18it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####3    | 535/1000 [03:43<05:22,  1.44it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####3    | 536/1000 [03:43<04:29,  1.72it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####3    | 537/1000 [03:43<04:28,  1.72it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####3    | 538/1000 [03:44<04:20,  1.77it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####3    | 539/1000 [03:44<03:55,  1.96it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####4    | 540/1000 [03:45<03:29,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####4    | 541/1000 [03:45<03:06,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####4    | 542/1000 [03:45<02:54,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####4    | 543/1000 [03:46<03:13,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  54%|#####4    | 544/1000 [03:46<03:30,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####4    | 545/1000 [03:47<03:09,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####4    | 546/1000 [03:47<02:51,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####4    | 547/1000 [03:48<03:45,  2.01it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####4    | 548/1000 [03:49<04:39,  1.62it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####4    | 549/1000 [03:49<04:05,  1.84it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####5    | 550/1000 [03:49<03:39,  2.05it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####5    | 551/1000 [03:50<03:46,  1.99it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####5    | 552/1000 [03:50<03:51,  1.93it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####5    | 553/1000 [03:51<03:50,  1.94it/s]Evaluating Finetuned wavlm_base_plus:  55%|#####5    | 554/1000 [03:51<03:56,  1.89it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####5    | 555/1000 [03:52<03:29,  2.12it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####5    | 556/1000 [03:52<03:03,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####5    | 557/1000 [03:53<03:14,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####5    | 558/1000 [03:53<03:16,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####5    | 559/1000 [03:54<03:28,  2.12it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 560/1000 [03:54<03:19,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 561/1000 [03:54<03:22,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 562/1000 [03:55<03:41,  1.97it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 563/1000 [03:56<03:40,  1.98it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 564/1000 [03:56<03:23,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  56%|#####6    | 565/1000 [03:56<03:24,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####6    | 566/1000 [03:57<03:37,  1.99it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####6    | 567/1000 [03:57<03:23,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####6    | 568/1000 [03:58<03:03,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####6    | 569/1000 [03:59<05:03,  1.42it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####6    | 570/1000 [03:59<04:12,  1.70it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####7    | 571/1000 [04:00<03:44,  1.91it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####7    | 572/1000 [04:00<03:26,  2.08it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####7    | 573/1000 [04:01<03:22,  2.11it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####7    | 574/1000 [04:01<03:06,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  57%|#####7    | 575/1000 [04:02<03:25,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####7    | 576/1000 [04:02<03:11,  2.21it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####7    | 577/1000 [04:02<02:54,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####7    | 578/1000 [04:06<10:02,  1.43s/it]Evaluating Finetuned wavlm_base_plus:  58%|#####7    | 579/1000 [04:06<07:47,  1.11s/it]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 580/1000 [04:07<06:12,  1.13it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 581/1000 [04:07<05:06,  1.37it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 582/1000 [04:08<04:21,  1.60it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 583/1000 [04:08<03:50,  1.81it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 584/1000 [04:08<03:27,  2.01it/s]Evaluating Finetuned wavlm_base_plus:  58%|#####8    | 585/1000 [04:09<03:18,  2.09it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####8    | 586/1000 [04:09<03:04,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####8    | 587/1000 [04:09<02:50,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####8    | 588/1000 [04:10<02:53,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####8    | 589/1000 [04:10<03:09,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####8    | 590/1000 [04:11<03:06,  2.19it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####9    | 591/1000 [04:12<04:18,  1.58it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####9    | 592/1000 [04:13<04:40,  1.45it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####9    | 593/1000 [04:13<03:58,  1.71it/s]Evaluating Finetuned wavlm_base_plus:  59%|#####9    | 594/1000 [04:14<03:52,  1.75it/s]Evaluating Finetuned wavlm_base_plus:  60%|#####9    | 595/1000 [04:14<03:24,  1.98it/s]Evaluating Finetuned wavlm_base_plus:  60%|#####9    | 596/1000 [04:15<03:28,  1.94it/s]Evaluating Finetuned wavlm_base_plus:  60%|#####9    | 597/1000 [04:15<03:10,  2.12it/s]Evaluating Finetuned wavlm_base_plus:  60%|#####9    | 598/1000 [04:15<02:57,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  60%|#####9    | 599/1000 [04:16<02:49,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 600/1000 [04:16<02:52,  2.31it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 601/1000 [04:16<02:39,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 602/1000 [04:17<02:29,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 603/1000 [04:17<02:19,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 604/1000 [04:17<02:13,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  60%|######    | 605/1000 [04:18<02:12,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  61%|######    | 606/1000 [04:18<02:18,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  61%|######    | 607/1000 [04:19<02:35,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  61%|######    | 608/1000 [04:19<02:32,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  61%|######    | 609/1000 [04:19<02:21,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  61%|######1   | 610/1000 [04:20<02:14,  2.90it/s]Evaluating Finetuned wavlm_base_plus:  61%|######1   | 611/1000 [04:20<02:22,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  61%|######1   | 612/1000 [04:20<02:19,  2.79it/s]Evaluating Finetuned wavlm_base_plus:  61%|######1   | 613/1000 [04:21<02:16,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  61%|######1   | 614/1000 [04:21<02:29,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  62%|######1   | 615/1000 [04:21<02:27,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  62%|######1   | 616/1000 [04:22<02:18,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  62%|######1   | 617/1000 [04:22<02:08,  2.97it/s]Evaluating Finetuned wavlm_base_plus:  62%|######1   | 618/1000 [04:22<02:13,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  62%|######1   | 619/1000 [04:23<02:05,  3.05it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 620/1000 [04:23<02:09,  2.93it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 621/1000 [04:24<02:17,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 622/1000 [04:24<02:12,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 623/1000 [04:24<02:05,  2.99it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 624/1000 [04:24<02:03,  3.05it/s]Evaluating Finetuned wavlm_base_plus:  62%|######2   | 625/1000 [04:25<02:09,  2.90it/s]Evaluating Finetuned wavlm_base_plus:  63%|######2   | 626/1000 [04:25<02:10,  2.87it/s]Evaluating Finetuned wavlm_base_plus:  63%|######2   | 627/1000 [04:26<02:15,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  63%|######2   | 628/1000 [04:26<02:14,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  63%|######2   | 629/1000 [04:26<02:28,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  63%|######3   | 630/1000 [04:27<02:29,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  63%|######3   | 631/1000 [04:27<02:39,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  63%|######3   | 632/1000 [04:28<02:38,  2.33it/s]Evaluating Finetuned wavlm_base_plus:  63%|######3   | 633/1000 [04:28<02:31,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  63%|######3   | 634/1000 [04:29<02:32,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  64%|######3   | 635/1000 [04:29<02:43,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  64%|######3   | 636/1000 [04:29<02:38,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  64%|######3   | 637/1000 [04:30<02:20,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  64%|######3   | 638/1000 [04:30<02:12,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  64%|######3   | 639/1000 [04:30<02:08,  2.81it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 640/1000 [04:31<02:13,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 641/1000 [04:31<02:16,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 642/1000 [04:32<02:29,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 643/1000 [04:32<02:39,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 644/1000 [04:33<02:47,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  64%|######4   | 645/1000 [04:33<02:46,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  65%|######4   | 646/1000 [04:34<02:39,  2.22it/s]Evaluating Finetuned wavlm_base_plus:  65%|######4   | 647/1000 [04:34<02:28,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  65%|######4   | 648/1000 [04:36<05:59,  1.02s/it]Evaluating Finetuned wavlm_base_plus:  65%|######4   | 649/1000 [04:41<11:52,  2.03s/it]Evaluating Finetuned wavlm_base_plus:  65%|######5   | 650/1000 [04:43<12:58,  2.22s/it]Evaluating Finetuned wavlm_base_plus:  65%|######5   | 651/1000 [04:44<10:13,  1.76s/it]Evaluating Finetuned wavlm_base_plus:  65%|######5   | 652/1000 [04:45<08:08,  1.40s/it]Evaluating Finetuned wavlm_base_plus:  65%|######5   | 653/1000 [04:45<06:12,  1.07s/it]Evaluating Finetuned wavlm_base_plus:  65%|######5   | 654/1000 [04:45<04:56,  1.17it/s]Evaluating Finetuned wavlm_base_plus:  66%|######5   | 655/1000 [04:46<03:55,  1.46it/s]Evaluating Finetuned wavlm_base_plus:  66%|######5   | 656/1000 [04:46<03:15,  1.76it/s]Evaluating Finetuned wavlm_base_plus:  66%|######5   | 657/1000 [04:46<02:49,  2.03it/s]Evaluating Finetuned wavlm_base_plus:  66%|######5   | 658/1000 [04:47<02:31,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  66%|######5   | 659/1000 [04:47<02:19,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 660/1000 [04:47<02:14,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 661/1000 [04:48<02:05,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 662/1000 [04:48<02:07,  2.66it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 663/1000 [04:48<02:04,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 664/1000 [04:49<02:07,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  66%|######6   | 665/1000 [04:49<02:14,  2.49it/s]Evaluating Finetuned wavlm_base_plus:  67%|######6   | 666/1000 [04:50<02:04,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  67%|######6   | 667/1000 [04:50<02:05,  2.65it/s]Evaluating Finetuned wavlm_base_plus:  67%|######6   | 668/1000 [04:50<01:56,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  67%|######6   | 669/1000 [04:51<01:53,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  67%|######7   | 670/1000 [04:51<02:51,  1.93it/s]Evaluating Finetuned wavlm_base_plus:  67%|######7   | 671/1000 [04:53<04:09,  1.32it/s]Evaluating Finetuned wavlm_base_plus:  67%|######7   | 672/1000 [04:54<04:16,  1.28it/s]Evaluating Finetuned wavlm_base_plus:  67%|######7   | 673/1000 [04:54<03:41,  1.48it/s]Evaluating Finetuned wavlm_base_plus:  67%|######7   | 674/1000 [04:54<03:15,  1.66it/s]Evaluating Finetuned wavlm_base_plus:  68%|######7   | 675/1000 [04:55<02:53,  1.88it/s]Evaluating Finetuned wavlm_base_plus:  68%|######7   | 676/1000 [04:55<02:57,  1.82it/s]Evaluating Finetuned wavlm_base_plus:  68%|######7   | 677/1000 [04:56<03:29,  1.54it/s]Evaluating Finetuned wavlm_base_plus:  68%|######7   | 678/1000 [04:57<03:49,  1.40it/s]Evaluating Finetuned wavlm_base_plus:  68%|######7   | 679/1000 [04:58<04:05,  1.31it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 680/1000 [04:59<04:06,  1.30it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 681/1000 [05:00<04:09,  1.28it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 682/1000 [05:00<03:52,  1.37it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 683/1000 [05:01<03:38,  1.45it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 684/1000 [05:01<03:11,  1.65it/s]Evaluating Finetuned wavlm_base_plus:  68%|######8   | 685/1000 [05:02<02:54,  1.80it/s]Evaluating Finetuned wavlm_base_plus:  69%|######8   | 686/1000 [05:03<03:24,  1.53it/s]Evaluating Finetuned wavlm_base_plus:  69%|######8   | 687/1000 [05:04<04:44,  1.10it/s]Evaluating Finetuned wavlm_base_plus:  69%|######8   | 688/1000 [05:05<04:52,  1.07it/s]Evaluating Finetuned wavlm_base_plus:  69%|######8   | 689/1000 [05:05<03:56,  1.32it/s]Evaluating Finetuned wavlm_base_plus:  69%|######9   | 690/1000 [05:06<03:21,  1.54it/s]Evaluating Finetuned wavlm_base_plus:  69%|######9   | 691/1000 [05:06<02:54,  1.77it/s]Evaluating Finetuned wavlm_base_plus:  69%|######9   | 692/1000 [05:07<02:51,  1.80it/s]Evaluating Finetuned wavlm_base_plus:  69%|######9   | 693/1000 [05:07<03:07,  1.64it/s]Evaluating Finetuned wavlm_base_plus:  69%|######9   | 694/1000 [05:08<02:53,  1.76it/s]Evaluating Finetuned wavlm_base_plus:  70%|######9   | 695/1000 [05:08<02:35,  1.96it/s]Evaluating Finetuned wavlm_base_plus:  70%|######9   | 696/1000 [05:09<02:25,  2.09it/s]Evaluating Finetuned wavlm_base_plus:  70%|######9   | 697/1000 [05:09<02:21,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  70%|######9   | 698/1000 [05:10<02:28,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  70%|######9   | 699/1000 [05:10<02:11,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 700/1000 [05:11<02:22,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 701/1000 [05:11<02:30,  1.99it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 702/1000 [05:12<02:21,  2.11it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 703/1000 [05:12<02:08,  2.31it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 704/1000 [05:12<02:01,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  70%|#######   | 705/1000 [05:13<01:57,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######   | 706/1000 [05:13<02:06,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######   | 707/1000 [05:14<02:17,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######   | 708/1000 [05:14<02:14,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######   | 709/1000 [05:14<02:04,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######1  | 710/1000 [05:15<02:04,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######1  | 711/1000 [05:15<01:56,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######1  | 712/1000 [05:16<01:53,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######1  | 713/1000 [05:16<01:53,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  71%|#######1  | 714/1000 [05:17<02:02,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######1  | 715/1000 [05:17<02:11,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######1  | 716/1000 [05:18<02:15,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######1  | 717/1000 [05:18<02:11,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######1  | 718/1000 [05:18<02:10,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######1  | 719/1000 [05:19<02:02,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 720/1000 [05:19<02:02,  2.29it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 721/1000 [05:20<02:12,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 722/1000 [05:20<02:02,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 723/1000 [05:21<02:09,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 724/1000 [05:21<02:05,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  72%|#######2  | 725/1000 [05:22<01:59,  2.29it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######2  | 726/1000 [05:22<02:06,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######2  | 727/1000 [05:22<01:56,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######2  | 728/1000 [05:26<06:30,  1.43s/it]Evaluating Finetuned wavlm_base_plus:  73%|#######2  | 729/1000 [05:26<04:52,  1.08s/it]Evaluating Finetuned wavlm_base_plus:  73%|#######3  | 730/1000 [05:27<03:49,  1.18it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######3  | 731/1000 [05:27<03:07,  1.43it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######3  | 732/1000 [05:27<02:37,  1.70it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######3  | 733/1000 [05:28<02:16,  1.96it/s]Evaluating Finetuned wavlm_base_plus:  73%|#######3  | 734/1000 [05:28<02:10,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######3  | 735/1000 [05:30<03:29,  1.26it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######3  | 736/1000 [05:30<02:58,  1.48it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######3  | 737/1000 [05:31<02:39,  1.65it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######3  | 738/1000 [05:31<02:21,  1.85it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######3  | 739/1000 [05:31<02:02,  2.13it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 740/1000 [05:32<01:58,  2.19it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 741/1000 [05:32<02:02,  2.11it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 742/1000 [05:33<02:02,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 743/1000 [05:33<01:50,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 744/1000 [05:34<01:58,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  74%|#######4  | 745/1000 [05:34<01:43,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######4  | 746/1000 [05:34<01:47,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######4  | 747/1000 [05:35<02:02,  2.06it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######4  | 748/1000 [05:36<02:15,  1.85it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######4  | 749/1000 [05:36<02:07,  1.97it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######5  | 750/1000 [05:36<01:58,  2.12it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######5  | 751/1000 [05:37<01:47,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######5  | 752/1000 [05:37<01:44,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######5  | 753/1000 [05:37<01:33,  2.65it/s]Evaluating Finetuned wavlm_base_plus:  75%|#######5  | 754/1000 [05:38<01:30,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######5  | 755/1000 [05:38<01:42,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######5  | 756/1000 [05:39<01:40,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######5  | 757/1000 [05:39<01:35,  2.55it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######5  | 758/1000 [05:40<01:57,  2.06it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######5  | 759/1000 [05:40<01:46,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 760/1000 [05:40<01:38,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 761/1000 [05:41<01:36,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 762/1000 [05:41<01:31,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 763/1000 [05:42<01:40,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 764/1000 [05:42<01:33,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  76%|#######6  | 765/1000 [05:42<01:29,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######6  | 766/1000 [05:43<01:24,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######6  | 767/1000 [05:43<01:19,  2.92it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######6  | 768/1000 [05:43<01:27,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######6  | 769/1000 [05:44<01:30,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######7  | 770/1000 [05:44<01:39,  2.31it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######7  | 771/1000 [05:45<01:36,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######7  | 772/1000 [05:45<01:33,  2.44it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######7  | 773/1000 [05:46<01:33,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  77%|#######7  | 774/1000 [05:46<01:29,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######7  | 775/1000 [05:46<01:32,  2.43it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######7  | 776/1000 [05:47<01:26,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######7  | 777/1000 [05:47<01:26,  2.59it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######7  | 778/1000 [05:48<01:33,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######7  | 779/1000 [05:48<01:46,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 780/1000 [05:49<01:50,  1.99it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 781/1000 [05:49<01:44,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 782/1000 [05:50<01:37,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 783/1000 [05:50<01:26,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 784/1000 [05:50<01:22,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  78%|#######8  | 785/1000 [05:51<01:26,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######8  | 786/1000 [05:51<01:22,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######8  | 787/1000 [05:51<01:32,  2.31it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######8  | 788/1000 [05:52<01:24,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######8  | 789/1000 [05:52<01:23,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######9  | 790/1000 [05:53<01:22,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######9  | 791/1000 [05:53<01:17,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######9  | 792/1000 [05:53<01:19,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######9  | 793/1000 [05:54<01:22,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  79%|#######9  | 794/1000 [05:54<01:15,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  80%|#######9  | 795/1000 [05:54<01:13,  2.81it/s]Evaluating Finetuned wavlm_base_plus:  80%|#######9  | 796/1000 [05:55<01:11,  2.84it/s]Evaluating Finetuned wavlm_base_plus:  80%|#######9  | 797/1000 [05:55<01:20,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  80%|#######9  | 798/1000 [05:56<01:15,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  80%|#######9  | 799/1000 [05:56<01:24,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 800/1000 [05:57<01:25,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 801/1000 [05:57<01:24,  2.36it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 802/1000 [05:57<01:22,  2.41it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 803/1000 [05:58<01:29,  2.21it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 804/1000 [05:58<01:30,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  80%|########  | 805/1000 [05:59<01:36,  2.01it/s]Evaluating Finetuned wavlm_base_plus:  81%|########  | 806/1000 [05:59<01:29,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  81%|########  | 807/1000 [06:00<01:19,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  81%|########  | 808/1000 [06:00<01:23,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  81%|########  | 809/1000 [06:01<01:22,  2.30it/s]Evaluating Finetuned wavlm_base_plus:  81%|########1 | 810/1000 [06:01<01:17,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  81%|########1 | 811/1000 [06:01<01:14,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  81%|########1 | 812/1000 [06:02<01:16,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  81%|########1 | 813/1000 [06:02<01:18,  2.37it/s]Evaluating Finetuned wavlm_base_plus:  81%|########1 | 814/1000 [06:03<01:15,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  82%|########1 | 815/1000 [06:03<01:19,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  82%|########1 | 816/1000 [06:03<01:14,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  82%|########1 | 817/1000 [06:04<01:13,  2.47it/s]Evaluating Finetuned wavlm_base_plus:  82%|########1 | 818/1000 [06:04<01:10,  2.56it/s]Evaluating Finetuned wavlm_base_plus:  82%|########1 | 819/1000 [06:04<01:09,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 820/1000 [06:05<01:10,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 821/1000 [06:05<01:18,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 822/1000 [06:06<01:22,  2.15it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 823/1000 [06:06<01:25,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 824/1000 [06:07<01:21,  2.15it/s]Evaluating Finetuned wavlm_base_plus:  82%|########2 | 825/1000 [06:07<01:15,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  83%|########2 | 826/1000 [06:08<01:14,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  83%|########2 | 827/1000 [06:08<01:13,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  83%|########2 | 828/1000 [06:08<01:10,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  83%|########2 | 829/1000 [06:09<01:05,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  83%|########2 | 830/1000 [06:09<01:04,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  83%|########3 | 831/1000 [06:10<01:04,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  83%|########3 | 832/1000 [06:10<01:15,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  83%|########3 | 833/1000 [06:11<01:20,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  83%|########3 | 834/1000 [06:11<01:24,  1.96it/s]Evaluating Finetuned wavlm_base_plus:  84%|########3 | 835/1000 [06:12<01:20,  2.05it/s]Evaluating Finetuned wavlm_base_plus:  84%|########3 | 836/1000 [06:12<01:28,  1.84it/s]Evaluating Finetuned wavlm_base_plus:  84%|########3 | 837/1000 [06:13<01:39,  1.63it/s]Evaluating Finetuned wavlm_base_plus:  84%|########3 | 838/1000 [06:14<01:37,  1.66it/s]Evaluating Finetuned wavlm_base_plus:  84%|########3 | 839/1000 [06:14<01:27,  1.85it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 840/1000 [06:14<01:18,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 841/1000 [06:15<01:13,  2.16it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 842/1000 [06:15<01:09,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 843/1000 [06:16<01:06,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 844/1000 [06:16<01:14,  2.10it/s]Evaluating Finetuned wavlm_base_plus:  84%|########4 | 845/1000 [06:17<01:44,  1.48it/s]Evaluating Finetuned wavlm_base_plus:  85%|########4 | 846/1000 [06:18<01:42,  1.51it/s]Evaluating Finetuned wavlm_base_plus:  85%|########4 | 847/1000 [06:18<01:29,  1.70it/s]Evaluating Finetuned wavlm_base_plus:  85%|########4 | 848/1000 [06:19<01:27,  1.74it/s]Evaluating Finetuned wavlm_base_plus:  85%|########4 | 849/1000 [06:20<01:24,  1.78it/s]Evaluating Finetuned wavlm_base_plus:  85%|########5 | 850/1000 [06:20<01:21,  1.83it/s]Evaluating Finetuned wavlm_base_plus:  85%|########5 | 851/1000 [06:21<01:17,  1.92it/s]Evaluating Finetuned wavlm_base_plus:  85%|########5 | 852/1000 [06:21<01:16,  1.94it/s]Evaluating Finetuned wavlm_base_plus:  85%|########5 | 853/1000 [06:21<01:08,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  85%|########5 | 854/1000 [06:23<01:57,  1.25it/s]Evaluating Finetuned wavlm_base_plus:  86%|########5 | 855/1000 [06:24<01:53,  1.28it/s]Evaluating Finetuned wavlm_base_plus:  86%|########5 | 856/1000 [06:24<01:53,  1.27it/s]Evaluating Finetuned wavlm_base_plus:  86%|########5 | 857/1000 [06:25<01:52,  1.27it/s]Evaluating Finetuned wavlm_base_plus:  86%|########5 | 858/1000 [06:26<01:40,  1.41it/s]Evaluating Finetuned wavlm_base_plus:  86%|########5 | 859/1000 [06:26<01:33,  1.51it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 860/1000 [06:27<01:24,  1.66it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 861/1000 [06:27<01:14,  1.86it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 862/1000 [06:28<01:06,  2.09it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 863/1000 [06:28<00:59,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 864/1000 [06:28<00:54,  2.51it/s]Evaluating Finetuned wavlm_base_plus:  86%|########6 | 865/1000 [06:29<00:51,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  87%|########6 | 866/1000 [06:29<00:49,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  87%|########6 | 867/1000 [06:29<00:52,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  87%|########6 | 868/1000 [06:30<00:50,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  87%|########6 | 869/1000 [06:30<00:46,  2.83it/s]Evaluating Finetuned wavlm_base_plus:  87%|########7 | 870/1000 [06:30<00:46,  2.82it/s]Evaluating Finetuned wavlm_base_plus:  87%|########7 | 871/1000 [06:31<00:49,  2.59it/s]Evaluating Finetuned wavlm_base_plus:  87%|########7 | 872/1000 [06:31<00:57,  2.24it/s]Evaluating Finetuned wavlm_base_plus:  87%|########7 | 873/1000 [06:33<01:43,  1.23it/s]Evaluating Finetuned wavlm_base_plus:  87%|########7 | 874/1000 [06:34<01:31,  1.38it/s]Evaluating Finetuned wavlm_base_plus:  88%|########7 | 875/1000 [06:34<01:16,  1.64it/s]Evaluating Finetuned wavlm_base_plus:  88%|########7 | 876/1000 [06:35<01:15,  1.64it/s]Evaluating Finetuned wavlm_base_plus:  88%|########7 | 877/1000 [06:36<01:59,  1.03it/s]Evaluating Finetuned wavlm_base_plus:  88%|########7 | 878/1000 [06:37<01:36,  1.27it/s]Evaluating Finetuned wavlm_base_plus:  88%|########7 | 879/1000 [06:37<01:16,  1.58it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 880/1000 [06:37<01:03,  1.90it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 881/1000 [06:38<01:01,  1.94it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 882/1000 [06:38<00:56,  2.09it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 883/1000 [06:39<00:53,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 884/1000 [06:39<00:53,  2.19it/s]Evaluating Finetuned wavlm_base_plus:  88%|########8 | 885/1000 [06:39<00:52,  2.17it/s]Evaluating Finetuned wavlm_base_plus:  89%|########8 | 886/1000 [06:40<00:52,  2.19it/s]Evaluating Finetuned wavlm_base_plus:  89%|########8 | 887/1000 [06:40<00:54,  2.09it/s]Evaluating Finetuned wavlm_base_plus:  89%|########8 | 888/1000 [06:41<00:54,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  89%|########8 | 889/1000 [06:41<00:53,  2.06it/s]Evaluating Finetuned wavlm_base_plus:  89%|########9 | 890/1000 [06:42<00:54,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  89%|########9 | 891/1000 [06:42<00:52,  2.07it/s]Evaluating Finetuned wavlm_base_plus:  89%|########9 | 892/1000 [06:43<00:54,  1.98it/s]Evaluating Finetuned wavlm_base_plus:  89%|########9 | 893/1000 [06:43<00:49,  2.18it/s]Evaluating Finetuned wavlm_base_plus:  89%|########9 | 894/1000 [06:44<00:46,  2.27it/s]Evaluating Finetuned wavlm_base_plus:  90%|########9 | 895/1000 [06:44<00:43,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  90%|########9 | 896/1000 [06:44<00:39,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  90%|########9 | 897/1000 [06:45<00:36,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  90%|########9 | 898/1000 [06:45<00:38,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  90%|########9 | 899/1000 [06:45<00:36,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 900/1000 [06:46<00:37,  2.69it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 901/1000 [06:46<00:36,  2.69it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 902/1000 [06:47<00:36,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 903/1000 [06:47<00:33,  2.86it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 904/1000 [06:47<00:31,  3.03it/s]Evaluating Finetuned wavlm_base_plus:  90%|######### | 905/1000 [06:48<00:32,  2.89it/s]Evaluating Finetuned wavlm_base_plus:  91%|######### | 906/1000 [06:48<00:34,  2.75it/s]Evaluating Finetuned wavlm_base_plus:  91%|######### | 907/1000 [06:48<00:33,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  91%|######### | 908/1000 [06:49<00:35,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  91%|######### | 909/1000 [06:49<00:36,  2.46it/s]Evaluating Finetuned wavlm_base_plus:  91%|#########1| 910/1000 [06:50<00:39,  2.29it/s]Evaluating Finetuned wavlm_base_plus:  91%|#########1| 911/1000 [06:50<00:38,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  91%|#########1| 912/1000 [06:51<00:37,  2.33it/s]Evaluating Finetuned wavlm_base_plus:  91%|#########1| 913/1000 [06:51<00:34,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  91%|#########1| 914/1000 [06:51<00:33,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########1| 915/1000 [06:51<00:29,  2.85it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########1| 916/1000 [06:52<00:28,  2.99it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########1| 917/1000 [06:52<00:26,  3.10it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########1| 918/1000 [06:52<00:25,  3.26it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########1| 919/1000 [06:53<00:24,  3.29it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 920/1000 [06:53<00:24,  3.22it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 921/1000 [06:53<00:23,  3.31it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 922/1000 [06:54<00:23,  3.32it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 923/1000 [06:54<00:26,  2.90it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 924/1000 [06:54<00:29,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  92%|#########2| 925/1000 [06:55<00:30,  2.48it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########2| 926/1000 [06:55<00:30,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########2| 927/1000 [06:56<00:34,  2.11it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########2| 928/1000 [06:56<00:32,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########2| 929/1000 [06:57<00:35,  1.98it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########3| 930/1000 [06:57<00:33,  2.08it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########3| 931/1000 [06:58<00:33,  2.04it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########3| 932/1000 [06:59<00:36,  1.89it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########3| 933/1000 [06:59<00:31,  2.14it/s]Evaluating Finetuned wavlm_base_plus:  93%|#########3| 934/1000 [06:59<00:27,  2.41it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 935/1000 [06:59<00:25,  2.60it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 936/1000 [07:00<00:25,  2.55it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 937/1000 [07:00<00:24,  2.55it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 938/1000 [07:01<00:23,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 939/1000 [07:01<00:27,  2.25it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########3| 940/1000 [07:02<00:25,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########4| 941/1000 [07:02<00:24,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########4| 942/1000 [07:02<00:22,  2.59it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########4| 943/1000 [07:03<00:25,  2.23it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########4| 944/1000 [07:03<00:23,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  94%|#########4| 945/1000 [07:04<00:23,  2.34it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########4| 946/1000 [07:04<00:22,  2.45it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########4| 947/1000 [07:05<00:22,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########4| 948/1000 [07:05<00:22,  2.32it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########4| 949/1000 [07:05<00:21,  2.38it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########5| 950/1000 [07:06<00:21,  2.28it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########5| 951/1000 [07:06<00:22,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########5| 952/1000 [07:07<00:21,  2.27it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########5| 953/1000 [07:07<00:21,  2.22it/s]Evaluating Finetuned wavlm_base_plus:  95%|#########5| 954/1000 [07:08<00:21,  2.18it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########5| 955/1000 [07:08<00:18,  2.42it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########5| 956/1000 [07:08<00:17,  2.50it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########5| 957/1000 [07:09<00:15,  2.69it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########5| 958/1000 [07:09<00:15,  2.67it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########5| 959/1000 [07:09<00:14,  2.74it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 960/1000 [07:10<00:13,  2.93it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 961/1000 [07:10<00:12,  3.17it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 962/1000 [07:10<00:13,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 963/1000 [07:11<00:16,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 964/1000 [07:11<00:15,  2.33it/s]Evaluating Finetuned wavlm_base_plus:  96%|#########6| 965/1000 [07:12<00:15,  2.20it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########6| 966/1000 [07:12<00:15,  2.26it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########6| 967/1000 [07:13<00:15,  2.18it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########6| 968/1000 [07:13<00:13,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########6| 969/1000 [07:14<00:12,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########7| 970/1000 [07:14<00:11,  2.54it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########7| 971/1000 [07:14<00:10,  2.70it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########7| 972/1000 [07:15<00:10,  2.72it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########7| 973/1000 [07:15<00:10,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  97%|#########7| 974/1000 [07:15<00:09,  2.61it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########7| 975/1000 [07:16<00:10,  2.40it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########7| 976/1000 [07:16<00:10,  2.35it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########7| 977/1000 [07:17<00:09,  2.39it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########7| 978/1000 [07:17<00:08,  2.71it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########7| 979/1000 [07:17<00:07,  2.73it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 980/1000 [07:18<00:07,  2.76it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 981/1000 [07:18<00:07,  2.66it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 982/1000 [07:19<00:07,  2.52it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 983/1000 [07:19<00:06,  2.63it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 984/1000 [07:19<00:05,  2.78it/s]Evaluating Finetuned wavlm_base_plus:  98%|#########8| 985/1000 [07:20<00:05,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########8| 986/1000 [07:20<00:05,  2.57it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########8| 987/1000 [07:20<00:04,  2.68it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########8| 988/1000 [07:21<00:04,  2.58it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########8| 989/1000 [07:21<00:04,  2.64it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########9| 990/1000 [07:22<00:03,  2.53it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########9| 991/1000 [07:22<00:03,  2.62it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########9| 992/1000 [07:22<00:03,  2.49it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########9| 993/1000 [07:23<00:02,  2.77it/s]Evaluating Finetuned wavlm_base_plus:  99%|#########9| 994/1000 [07:23<00:02,  2.87it/s]Evaluating Finetuned wavlm_base_plus: 100%|#########9| 995/1000 [07:23<00:01,  2.80it/s]Evaluating Finetuned wavlm_base_plus: 100%|#########9| 996/1000 [07:24<00:01,  2.69it/s]Evaluating Finetuned wavlm_base_plus: 100%|#########9| 997/1000 [07:25<00:01,  1.97it/s]Evaluating Finetuned wavlm_base_plus: 100%|#########9| 998/1000 [07:25<00:00,  2.27it/s]Evaluating Finetuned wavlm_base_plus: 100%|#########9| 999/1000 [07:25<00:00,  2.38it/s]Evaluating Finetuned wavlm_base_plus: 100%|##########| 1000/1000 [07:26<00:00,  2.62it/s]Evaluating Finetuned wavlm_base_plus: 100%|##########| 1000/1000 [07:26<00:00,  2.24it/s]
2025-03-30 20:24:07,164 - __main__ - INFO - Evaluation results for Finetuned wavlm_base_plus on 4000 trials:
2025-03-30 20:24:07,169 - __main__ - INFO -   EER: 36.72%
2025-03-30 20:24:07,169 - __main__ - INFO -   TAR@1%FAR: 5.20%
2025-03-30 20:24:07,169 - __main__ - INFO -   Accuracy: 63.28%
2025-03-30 20:24:07,176 - __main__ - ERROR - CRITICAL: Pretrained and finetuned models have identical EER values!
2025-03-30 20:24:07,176 - __main__ - ERROR - This suggests there might be an issue with the evaluation code or model loading.
2025-03-30 20:24:07,281 - __main__ - INFO - Results saved to models\speaker_verification\wavlm_ft_fixed\comparison_results.csv
2025-03-30 20:24:08,010 - __main__ - INFO - Comparison plot saved to models\speaker_verification\wavlm_ft_fixed\comparison_plot.png
Traceback (most recent call last):
  File "C:\Users\SQREAM\Desktop\semester_six\speech_understanding_assignment_02\src\speaker_verification\evaluate_models.py", line 975, in <module>
    main()
  File "C:\Users\SQREAM\Desktop\semester_six\speech_understanding_assignment_02\src\speaker_verification\evaluate_models.py", line 960, in main
    compare_pretrained_and_finetuned(
  File "C:\Users\SQREAM\Desktop\semester_six\speech_understanding_assignment_02\src\speaker_verification\evaluate_models.py", line 837, in compare_pretrained_and_finetuned
    sample_item = dataset[0]  # First item from your evaluation dataset
NameError: name 'dataset' is not defined
